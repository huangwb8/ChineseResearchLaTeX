# LaTeX æ ‡ä¹¦æ™ºèƒ½è¿ç§»æŠ€èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

**æ–¹æ¡ˆç¼–å·**: v202601081002
**åˆ›å»ºæ—¶é—´**: 2026-01-08
**ç›®æ ‡ç‰ˆæœ¬**: v1.3.0
**å½“å‰ç‰ˆæœ¬**: v1.2.0
**çŠ¶æ€**: å¾…è¯„å®¡

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### ç°çŠ¶è¯„ä¼°

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **æ¶æ„è®¾è®¡** | â­â­â­â­ | äº”é˜¶æ®µå·¥ä½œæµæ¸…æ™°ï¼Œæ¨¡å—èŒè´£æ˜ç¡® |
| **æ–‡æ¡£å®Œæ•´æ€§** | â­â­â­â­â­ | è¶…è¿‡ 2000 è¡Œå‚è€ƒæ–‡æ¡£ï¼Œè¦†ç›–æ‰€æœ‰åœºæ™¯ |
| **å®‰å…¨æ€§** | â­â­â­â­â­ | ç™½åå•æœºåˆ¶ + è‡ªåŠ¨å¿«ç…§ + ä¸€é”®æ¢å¤ |
| **æ€§èƒ½** | â­â­â­ | å¤§å‹é¡¹ç›®å¯èƒ½è€—æ—¶ 30+ åˆ†é’Ÿï¼ˆAI è°ƒç”¨ç“¶é¢ˆï¼‰ |
| **å¯æµ‹è¯•æ€§** | â­â­ | ç¼ºå°‘è‡ªåŠ¨åŒ–æµ‹è¯•ï¼Œé•¿æœŸå¯ç»´æŠ¤æ€§é£é™© |

### å…³é”®é—®é¢˜

| ä¼˜å…ˆçº§ | é—®é¢˜ | å½±å“ |
|--------|------|------|
| **ğŸ”´ é«˜** | AI è¯­ä¹‰æ˜ å°„æ€§èƒ½ç“¶é¢ˆ | å¤§å‹é¡¹ç›®è€—æ—¶ 30+ åˆ†é’Ÿ |
| **ğŸ”´ é«˜** | ç¼ºå°‘æµ‹è¯•è¦†ç›– | ä»£ç è´¨é‡éš¾ä»¥ä¿è¯ |
| **ğŸ”´ é«˜** | é”™è¯¯æ¢å¤æœºåˆ¶ä¸å®Œå–„ | æ— æ³•é’ˆå¯¹å•ä¸ªæ–‡ä»¶å›æ»š |
| **ğŸŸ¡ ä¸­** | é…ç½®å¤æ‚åº¦è¿‡é«˜ | æ–°ç”¨æˆ·ä¸Šæ‰‹å›°éš¾ |
| **ğŸŸ¡ ä¸­** | ç¼ºå°‘è¿›åº¦åé¦ˆ | "é»‘ç›’"æ“ä½œä½“éªŒå·® |
| **ğŸŸ¡ ä¸­** | ç‰ˆæœ¬å…¼å®¹æ€§æ£€æµ‹ä¸è¶³ | å¯èƒ½äº§ç”Ÿéšå¼é”™è¯¯ |

---

## ğŸ¯ ä¼˜åŒ–æ–¹æ¡ˆè¯¦è§£

### ä¼˜åŒ–é¡¹ 1: å¼•å…¥åˆ†å±‚ç¼“å­˜æœºåˆ¶

**ä¼˜å…ˆçº§**: ğŸ”´ é«˜
**å®æ–½éš¾åº¦**: â­â­ ä¸­ç­‰
**é¢„æœŸæ•ˆæœ**: æ€§èƒ½æå‡ 5-10 å€

#### é—®é¢˜æè¿°

å½“å‰ AI è¯­ä¹‰æ˜ å°„å­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼š
- æ¯å¯¹æ–‡ä»¶éƒ½éœ€è¦è°ƒç”¨ AI åˆ¤æ–­æ˜ å°„å…³ç³»
- å¤§å‹é¡¹ç›®ï¼ˆ100+ æ–‡ä»¶ï¼‰å¯èƒ½éœ€è¦æ•°ç™¾æ¬¡ AI è°ƒç”¨
- æ¯æ¬¡è°ƒç”¨è€—æ—¶ 5-10 ç§’ï¼Œæ€»è®¡å¯èƒ½è¶…è¿‡ 30 åˆ†é’Ÿ

#### è§£å†³æ–¹æ¡ˆ

å¼•å…¥ä¸‰å±‚ç¼“å­˜æœºåˆ¶ï¼š

```python
# core/cache_manager.pyï¼ˆæ–°å»ºï¼‰

class CacheManager:
    """åˆ†å±‚ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # L1: å†…å­˜ç¼“å­˜ï¼ˆå½“å‰ä¼šè¯ï¼‰
        self.memory_cache = {}

        # L2: SQLite ç£ç›˜ç¼“å­˜ï¼ˆè·¨ä¼šè¯ï¼‰
        self.db_path = self.cache_dir / "mapping_cache.db"
        self.conn = sqlite3.connect(self.db_path)
        self._init_db()

    def get(self, old_file: str, new_file: str) -> Optional[dict]:
        """è·å–ç¼“å­˜ï¼ˆL1 â†’ L2 â†’ AIï¼‰"""
        cache_key = self._make_key(old_file, new_file)

        # L1: å†…å­˜ç¼“å­˜
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]

        # L2: SQLite ç¼“å­˜
        cursor = self.conn.execute(
            "SELECT result FROM mapping_cache WHERE cache_key = ?",
            (cache_key,)
        )
        row = cursor.fetchone()
        if row:
            result = json.loads(row[0])
            self.memory_cache[cache_key] = result  # æå‡åˆ° L1
            return result

        return None

    def set(self, old_file: str, new_file: str, result: dict):
        """è®¾ç½®ç¼“å­˜ï¼ˆåŒæ—¶å†™å…¥ L1 å’Œ L2ï¼‰"""
        cache_key = self._make_key(old_file, new_file)

        # å†™å…¥ L1
        self.memory_cache[cache_key] = result

        # å†™å…¥ L2
        self.conn.execute(
            "INSERT OR REPLACE INTO mapping_cache (cache_key, result) VALUES (?, ?)",
            (cache_key, json.dumps(result))
        )
        self.conn.commit()
```

#### é…ç½®å˜æ›´

```yaml
# config.yaml æ–°å¢

cache:
  enabled: true                    # å¯ç”¨ç¼“å­˜
  strategy: "layered"              # åˆ†å±‚ç­–ç•¥: layered/none
  memory_max_size: 1000            # å†…å­˜ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
  disk_db_path: "cache/mapping_cache.db"  # ç£ç›˜ç¼“å­˜è·¯å¾„
  ttl_days: 30                     # ç¼“å­˜è¿‡æœŸå¤©æ•°
```

#### é›†æˆåˆ° mapping_engine.py

```python
class MappingEngine:
    def __init__(self, config: dict):
        self.config = config
        self.cache = CacheManager(config['cache']['disk_db_path'])

    async def _ai_judge_mapping(self, context: dict) -> dict:
        # æ£€æŸ¥ç¼“å­˜
        cached = self.cache.get(context['old_file'], context['new_file'])
        if cached:
            return cached

        # è°ƒç”¨ AI
        result = await self._call_ai(context)

        # å†™å…¥ç¼“å­˜
        self.cache.set(context['old_file'], context['new_file'], result)

        return result
```

---

### ä¼˜åŒ–é¡¹ 2: æ‰¹é‡ AI è°ƒç”¨ä¼˜åŒ–

**ä¼˜å…ˆçº§**: ğŸ”´ é«˜
**å®æ–½éš¾åº¦**: â­â­ ä¸­ç­‰
**é¢„æœŸæ•ˆæœ**: å‡å°‘ç½‘ç»œå¼€é”€ï¼Œæå‡ 3-5 å€

#### é—®é¢˜æè¿°

å½“å‰æ¯å¯¹æ–‡ä»¶ç‹¬ç«‹è°ƒç”¨ AIï¼Œå­˜åœ¨å¤§é‡ç½‘ç»œå¼€é”€ã€‚

#### è§£å†³æ–¹æ¡ˆ

å®ç°æ‰¹é‡è°ƒç”¨ï¼š

```python
class MappingEngine:
    async def batch_ai_judge(self, contexts: List[dict]) -> List[dict]:
        """æ‰¹é‡åˆ¤æ–­æ˜ å°„å…³ç³»ï¼ˆå•æ¬¡ API è°ƒç”¨ï¼‰"""
        # æ„å»ºæ‰¹é‡ prompt
        batch_prompt = self._build_batch_prompt(contexts)

        # å•æ¬¡ API è°ƒç”¨
        response = await self.ai_client.complete(
            prompt=batch_prompt,
            max_tokens=16000  # è¶³å¤Ÿå¤„ç†å¤šä¸ªç»“æœ
        )

        # è§£ææ‰¹é‡ç»“æœ
        results = self._parse_batch_results(response)

        return results

    def _build_batch_prompt(self, contexts: List[dict]) -> str:
        """æ„å»ºæ‰¹é‡åˆ¤æ–­ prompt"""
        prompt = "è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡ä»¶å¯¹æ˜¯å¦åº”è¯¥æ˜ å°„ï¼Œè¿”å› JSON æ•°ç»„ï¼š\n\n"
        for i, ctx in enumerate(contexts):
            prompt += f"\n## æ–‡ä»¶å¯¹ {i+1}\n"
            prompt += f"æ—§æ–‡ä»¶: {ctx['old_file']}\n"
            prompt += f"æ–°æ–‡ä»¶: {ctx['new_file']}\n"
            prompt += f"å†…å®¹é¢„è§ˆ: {ctx['old_content'][:200]}...\n"
            prompt += f"å†…å®¹é¢„è§ˆ: {ctx['new_content'][:200]}...\n"

        prompt += "\n## è¾“å‡ºæ ¼å¼\n"
        prompt += '''
[
  {"should_map": true, "confidence": "high", "score": 0.95, "reason": "..."},
  {"should_map": false, "confidence": "low", "score": 0.3, "reason": "..."}
]
'''
        return prompt
```

#### é…ç½®å˜æ›´

```yaml
# config.yaml æ–°å¢

ai:
  # æ‰¹é‡è°ƒç”¨é…ç½®
  batch_mode: true                # å¯ç”¨æ‰¹é‡æ¨¡å¼
  batch_size: 10                  # æ¯æ‰¹å¤„ç†æ•°é‡
  max_concurrent_batches: 3       # æœ€å¤§å¹¶å‘æ‰¹æ¬¡æ•°
```

---

### ä¼˜åŒ–é¡¹ 3: å¹¶è¡ŒåŒ–å¤„ç†

**ä¼˜å…ˆçº§**: ğŸ”´ é«˜
**å®æ–½éš¾åº¦**: â­â­ ä¸­ç­‰
**é¢„æœŸæ•ˆæœ**: å¤šæ ¸ CPU åœºæ™¯ä¸‹æå‡ 2-4 å€

#### é—®é¢˜æè¿°

å½“å‰å¤„ç†æµç¨‹ä¸²è¡Œæ‰§è¡Œï¼Œæ— æ³•åˆ©ç”¨å¤šæ ¸ CPUã€‚

#### è§£å†³æ–¹æ¡ˆ

ä½¿ç”¨ asyncio + ThreadPoolExecutor å®ç°å¹¶è¡Œï¼š

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class MappingEngine:
    def __init__(self, config: dict):
        self.executor = ThreadPoolExecutor(max_workers=config['ai']['max_workers'])

    async def compute_structure_diff(self, old_sections, new_sections):
        """å¹¶è¡Œè®¡ç®—æ‰€æœ‰æ–‡ä»¶å¯¹çš„æ˜ å°„å…³ç³»"""
        tasks = []

        # ä¸ºæ¯å¯¹æ–‡ä»¶åˆ›å»ºä»»åŠ¡
        for old_section in old_sections:
            for new_section in new_sections:
                task = self._judge_pair(old_section, new_section)
                tasks.append(task)

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks)

        return self._aggregate_results(results)
```

#### é…ç½®å˜æ›´

```yaml
# config.yaml æ–°å¢

ai:
  max_workers: 4                  # æœ€å¤§å¹¶è¡Œ worker æ•°ï¼ˆé»˜è®¤ CPU æ ¸å¿ƒæ•°ï¼‰
```

---

### ä¼˜åŒ–é¡¹ 4: å»ºç«‹æµ‹è¯•ä½“ç³»

**ä¼˜å…ˆçº§**: ğŸ”´ é«˜
**å®æ–½éš¾åº¦**: â­â­â­ å¤æ‚
**é¢„æœŸæ•ˆæœ**: è¦†ç›–ç‡ 80%ï¼Œé•¿æœŸå¯ç»´æŠ¤æ€§æå‡

#### é—®é¢˜æè¿°

å½“å‰æ— è‡ªåŠ¨åŒ–æµ‹è¯•ï¼Œä»£ç è´¨é‡éš¾ä»¥ä¿è¯ã€‚

#### è§£å†³æ–¹æ¡ˆ

å»ºç«‹ pytest æµ‹è¯•æ¡†æ¶ï¼š

```python
# tests/test_mapping_engine.pyï¼ˆæ–°å»ºï¼‰

import pytest
from core.mapping_engine import MappingEngine

@pytest.fixture
def engine():
    config = load_config("config.yaml")
    return MappingEngine(config)

def test_high_confidence_mapping(engine):
    """æµ‹è¯•é«˜ç½®ä¿¡åº¦ä¸€å¯¹ä¸€æ˜ å°„"""
    old_file = create_test_file("ç«‹é¡¹ä¾æ®.tex", "è¿™æ˜¯ç«‹é¡¹ä¾æ®å†…å®¹")
    new_file = create_test_file("ç«‹é¡¹ä¾æ®.tex", "è¿™æ˜¯ç«‹é¡¹ä¾æ®å†…å®¹")

    result = engine._ai_judge_mapping({
        'old_file': old_file,
        'new_file': new_file
    })

    assert result['should_map'] == True
    assert result['confidence'] == 'high'
    assert result['score'] >= 0.85

def test_low_confidence_mapping(engine):
    """æµ‹è¯•ä½ç½®ä¿¡åº¦æ˜ å°„ï¼ˆéœ€äººå·¥ç¡®è®¤ï¼‰"""
    old_file = create_test_file("ç ”ç©¶å†…å®¹.tex", "ç ”ç©¶å†…å®¹...")
    new_file = create_test_file("å·¥ä½œæ¡ä»¶.tex", "å·¥ä½œæ¡ä»¶...")

    result = engine._ai_judge_mapping({
        'old_file': old_file,
        'new_file': new_file
    })

    assert result['should_map'] == False
    assert result['confidence'] == 'low'
```

#### æµ‹è¯•è¦†ç›–ç›®æ ‡

| æ¨¡å— | ç›®æ ‡è¦†ç›–ç‡ | å…³é”®æµ‹è¯•ç‚¹ |
|------|-----------|-----------|
| `config_loader.py` | 90% | é…ç½®åŠ è½½ã€è·¯å¾„è§£æã€é»˜è®¤å€¼ |
| `project_analyzer.py` | 85% | ç« èŠ‚æ ‘è§£æã€æ ‡ç­¾æå–ã€ç‰ˆæœ¬è¯†åˆ« |
| `mapping_engine.py` | 80% | AI æ˜ å°„ã€ç¼“å­˜æœºåˆ¶ã€æ‰¹é‡è°ƒç”¨ |
| `migration_plan.py` | 75% | è®¡åˆ’ç”Ÿæˆã€ä»»åŠ¡åˆ†è§£ |
| `migrator.py` | 70% | å†…å®¹è¿ç§»ã€èµ„æºæ–‡ä»¶å¤„ç† |
| `compiler.py` | 80% | 4æ­¥ç¼–è¯‘ã€é”™è¯¯æå– |

---

### ä¼˜åŒ–é¡¹ 5: ç»†ç²’åº¦é”™è¯¯æ¢å¤

**ä¼˜å…ˆçº§**: ğŸ”´ é«˜
**å®æ–½éš¾åº¦**: â­â­ ä¸­ç­‰
**é¢„æœŸæ•ˆæœ**: æ”¯æŒå•æ–‡ä»¶å›æ»šï¼Œæå‡å¯é æ€§

#### é—®é¢˜æè¿°

å½“å‰åªæ”¯æŒ"å…¨éƒ¨æ¢å¤"ï¼Œæ— æ³•é’ˆå¯¹å•ä¸ªæ–‡ä»¶å›æ»šã€‚

#### è§£å†³æ–¹æ¡ˆ

å®ç°åŸºäº Git çš„ç»†ç²’åº¦ç‰ˆæœ¬æ§åˆ¶ï¼š

```python
# core/rollback_manager.pyï¼ˆæ–°å»ºï¼‰

class RollbackManager:
    """ç»†ç²’åº¦å›æ»šç®¡ç†å™¨"""

    def __init__(self, project_path: str, backup_dir: str):
        self.project_path = Path(project_path)
        self.backup_dir = Path(backup_dir)

    def create_file_snapshot(self, file_path: str) -> str:
        """ä¸ºå•ä¸ªæ–‡ä»¶åˆ›å»ºå¿«ç…§"""
        file_path = Path(file_path)
        snapshot_name = f"{file_path.name}_{uuid.uuid4().hex[:8]}.bak"
        snapshot_path = self.backup_dir / snapshot_name

        shutil.copy2(file_path, snapshot_path)

        return snapshot_path

    def restore_file(self, file_path: str, snapshot_name: str):
        """æ¢å¤å•ä¸ªæ–‡ä»¶"""
        snapshot_path = self.backup_dir / snapshot_name
        shutil.copy2(snapshot_path, file_path)

    def restore_all(self, run_id: str):
        """æ¢å¤æ‰€æœ‰æ–‡ä»¶ï¼ˆåŸæœ‰åŠŸèƒ½ï¼‰"""
        backup_path = self.backup_dir / f"run_{run_id}"
        shutil.copytree(backup_path, self.project_path, dirs_exist_ok=True)
```

---

### ä¼˜åŒ–é¡¹ 6: å®æ—¶è¿›åº¦åé¦ˆ

**ä¼˜å…ˆçº§**: ğŸŸ¡ ä¸­
**å®æ–½éš¾åº¦**: â­ ç®€å•
**é¢„æœŸæ•ˆæœ**: ç”¨æˆ·ä½“éªŒæ˜¾è‘—æå‡

#### é—®é¢˜æè¿°

å½“å‰æ˜¯"é»‘ç›’"æ“ä½œï¼Œç”¨æˆ·çœ‹ä¸åˆ°å®æ—¶è¿›åº¦ã€‚

#### è§£å†³æ–¹æ¡ˆ

ä½¿ç”¨ rich.progress å®ç°è¿›åº¦æ¡ï¼š

```python
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn

class MigrationOrchestrator:
    def run_migration(self, old_project, new_project):
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            expand=True
        ) as progress:

            # Phase 1: åˆ†æç»“æ„
            task1 = progress.add_task("[cyan]åˆ†ææ—§é¡¹ç›®ç»“æ„...", total=100)
            self.analyze_project(old_project)
            progress.update(task1, completed=100)

            # Phase 2: è®¡ç®—å·®å¼‚
            task2 = progress.add_task("[cyan]è®¡ç®—ç»“æ„å·®å¼‚...", total=100)
            self.compute_diff(old_project, new_project)
            progress.update(task2, completed=100)

            # Phase 3: ç”Ÿæˆè¿ç§»è®¡åˆ’
            task3 = progress.add_task("[cyan]ç”Ÿæˆè¿ç§»è®¡åˆ’...", total=100)
            plan = self.build_plan(diff_result)
            progress.update(task3, completed=100)

            # Phase 4: æ‰§è¡Œè¿ç§»ï¼ˆæ˜¾ç¤ºè¯¦ç»†è¿›åº¦ï¼‰
            task4 = progress.add_task("[cyan]è¿ç§»å†…å®¹...", total=len(plan.tasks))
            for task in plan.tasks:
                self.execute_task(task)
                progress.update(task4, advance=1, description=f"[cyan]è¿ç§»: {task.description}")
```

---

### ä¼˜åŒ–é¡¹ 7: ç®€åŒ–é…ç½®ï¼ˆé¢„è®¾æ¨¡æ¿ï¼‰

**ä¼˜å…ˆçº§**: ğŸŸ¡ ä¸­
**å®æ–½éš¾åº¦**: â­ ç®€å•
**é¢„æœŸæ•ˆæœ**: æ–°ç”¨æˆ·ä¸Šæ‰‹æ—¶é—´ä» 30 åˆ†é’Ÿé™è‡³ 5 åˆ†é’Ÿ

#### é—®é¢˜æè¿°

å½“å‰ 400+ è¡Œé…ç½®ï¼Œæ–°ç”¨æˆ·ä¸Šæ‰‹å›°éš¾ã€‚

#### è§£å†³æ–¹æ¡ˆ

æä¾›é¢„è®¾æ¨¡æ¿ï¼š

```yaml
# config.yaml æ–°å¢

profiles:
  quick:
    description: "å¿«é€Ÿæ¨¡å¼ï¼ˆé€‚åˆå°é¡¹ç›®ï¼Œ<20 ä¸ªæ–‡ä»¶ï¼‰"
    ai:
      batch_mode: false
      max_workers: 2
    cache:
      enabled: false
    optimization:
      max_rounds: 3

  balanced:
    description: "å¹³è¡¡æ¨¡å¼ï¼ˆé€‚åˆä¸­å‹é¡¹ç›®ï¼Œ20-100 ä¸ªæ–‡ä»¶ï¼‰"
    ai:
      batch_mode: true
      batch_size: 10
      max_workers: 4
    cache:
      enabled: true
    optimization:
      max_rounds: 5

  thorough:
    description: "ç²¾ç¡®æ¨¡å¼ï¼ˆé€‚åˆå¤§å‹é¡¹ç›®ï¼Œ>100 ä¸ªæ–‡ä»¶ï¼‰"
    ai:
      batch_mode: true
      batch_size: 20
      max_workers: 8
    cache:
      enabled: true
    optimization:
      max_rounds: 7
```

#### CLI ä½¿ç”¨

```bash
# ä½¿ç”¨é¢„è®¾æ¨¡æ¿
python scripts/run.py apply --profile quick --old ... --new ...

# è¦†ç›–ç‰¹å®šé…ç½®
python scripts/run.py apply --profile balanced --ai.max_workers 8 --old ... --new ...
```

---

### ä¼˜åŒ–é¡¹ 8: æ™ºèƒ½ç‰ˆæœ¬æ£€æµ‹

**ä¼˜å…ˆçº§**: ğŸŸ¡ ä¸­
**å®æ–½éš¾åº¦**: â­â­ ä¸­ç­‰
**é¢„æœŸæ•ˆæœ**: é¿å…éšå¼é”™è¯¯ï¼Œæå‰é¢„è­¦

#### é—®é¢˜æè¿°

å½“å‰ä¸ä¼šä¸»åŠ¨æ£€æµ‹ç‰ˆæœ¬å†²çªï¼Œå¯èƒ½å¯¼è‡´éšå¼é”™è¯¯ã€‚

#### è§£å†³æ–¹æ¡ˆ

å¢å¼ºç‰ˆæœ¬æ£€æµ‹ï¼š

```python
class ProjectAnalyzer:
    def detect_project_version(self, project_path: str) -> str:
        """æ£€æµ‹é¡¹ç›®ç‰ˆæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        main_tex = Path(project_path) / "main.tex"

        # è§£æ main.tex
        with open(main_tex, 'r', encoding='utf-8') as f:
            content = f.read()

        # æ–¹æ³•1: æ£€æŸ¥ç‰ˆæœ¬æ³¨é‡Š
        version_match = re.search(r'%\s*Version:\s*(\d{4})', content)
        if version_match:
            return version_match.group(1)

        # æ–¹æ³•2: æ£€æŸ¥æ¿å—ç»“æ„ç‰¹å¾
        if r'\section{ï¼ˆä¸€ï¼‰' in content:
            return "2026"
        elif r'\section{ä¸€ã€' in content:
            return "2025"
        elif r'\section{1}' in content:
            return "2024"

        return "unknown"

    def check_version_compatibility(self, old_version: str, new_version: str) -> dict:
        """æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§"""
        compatibility_map = {
            ("2024", "2025"): {"status": "compatible", "notes": "ç« èŠ‚ç¼–å·ä½“ç³»å˜åŒ–"},
            ("2025", "2026"): {"status": "compatible", "notes": "æ¿å—é‡ç»„ï¼Œæ–°å¢é£é™©åº”å¯¹ç« èŠ‚"},
            ("2024", "2026"): {"status": "requires_review", "notes": "è·¨ç‰ˆæœ¬è¿ç§»ï¼Œå»ºè®®åˆ†æ­¥è¿›è¡Œ"},
        }

        key = (old_version, new_version)
        if key in compatibility_map:
            return compatibility_map[key]

        return {"status": "unknown", "notes": "æœªçŸ¥ç‰ˆæœ¬ç»„åˆ"}
```

---

### ä¼˜åŒ–é¡¹ 9: AI å†™ä½œé£æ ¼è¯„åˆ†

**ä¼˜å…ˆçº§**: ğŸŸ¢ ä½
**å®æ–½éš¾åº¦**: â­â­ ä¸­ç­‰
**é¢„æœŸæ•ˆæœ**: è¿ç§»å†…å®¹è´¨é‡æå‡ 20%

#### è§£å†³æ–¹æ¡ˆ

åœ¨ Phase 5 è¿­ä»£ä¼˜åŒ–ä¸­å¢åŠ å†™ä½œé£æ ¼è¯„åˆ†ï¼š

```python
class WritingStyleScorer:
    """AI å†™ä½œé£æ ¼è¯„åˆ†å™¨"""

    def score_section(self, content: str, section_type: str) -> dict:
        """å¯¹ç« èŠ‚å†…å®¹è¿›è¡Œé£æ ¼è¯„åˆ†"""
        criteria = {
            "clarity": "æ¸…æ™°åº¦ï¼ˆè¡¨è¿°æ˜¯å¦æ˜ç¡®ï¼‰",
            "conciseness": "ç®€æ´æ€§ï¼ˆæœ‰æ— å†—ä½™è¡¨è¿°ï¼‰",
            "coherence": "è¿è´¯æ€§ï¼ˆé€»è¾‘æ˜¯å¦æµç•…ï¼‰",
            "academic_tone": "å­¦æœ¯æ€§ï¼ˆæ˜¯å¦ç¬¦åˆå­¦æœ¯è§„èŒƒï¼‰"
        }

        scores = {}
        for criterion, description in criteria.items():
            scores[criterion] = self._ai_score_criterion(content, section_type, criterion)

        return {
            "scores": scores,
            "average": sum(scores.values()) / len(scores),
            "suggestions": self._generate_suggestions(scores)
        }
```

---

### ä¼˜åŒ–é¡¹ 10: æ’ä»¶åŒ–æ¶æ„

**ä¼˜å…ˆçº§**: ğŸŸ¢ ä½
**å®æ–½éš¾åº¦**: â­â­â­ å¤æ‚
**é¢„æœŸæ•ˆæœ**: æ”¯æŒè‡ªå®šä¹‰è¿ç§»ç­–ç•¥

#### è§£å†³æ–¹æ¡ˆ

å®ç°æ’ä»¶ç³»ç»Ÿï¼š

```python
# core/plugin_manager.pyï¼ˆæ–°å»ºï¼‰

class PluginManager:
    """æ’ä»¶ç®¡ç†å™¨"""

    def __init__(self, plugin_dir: str = "plugins"):
        self.plugin_dir = Path(plugin_dir)
        self.plugins = {}

    def load_plugins(self):
        """åŠ è½½æ‰€æœ‰æ’ä»¶"""
        for plugin_file in self.plugin_dir.glob("*.py"):
            module = import_module(plugin_file.stem)
            if hasattr(module, 'register'):
                plugin = module.register()
                self.plugins[plugin.name] = plugin

    def get_plugin(self, name: str):
        """è·å–æ’ä»¶"""
        return self.plugins.get(name)
```

#### æ’ä»¶ç¤ºä¾‹

```python
# plugins/custom_mapping.pyï¼ˆç¤ºä¾‹ï¼‰

def register():
    return CustomMappingPlugin()

class CustomMappingPlugin:
    name = "custom_mapping"

    def on_mapping(self, old_file, new_file):
        # è‡ªå®šä¹‰æ˜ å°„é€»è¾‘
        if "ç‰¹æ®Š" in old_file:
            return {"should_map": True, "strategy": "custom"}
        return None
```

---

### ä¼˜åŒ–é¡¹ 11: æ€§èƒ½ç›‘æ§ä¸åˆ†æ

**ä¼˜å…ˆçº§**: ğŸŸ¢ ä½
**å®æ–½éš¾åº¦**: â­ ç®€å•
**é¢„æœŸæ•ˆæœ**: è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ

#### è§£å†³æ–¹æ¡ˆ

é›†æˆæ€§èƒ½åˆ†æï¼š

```python
import time
from functools import wraps

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.metrics = {}

    def record(self, func):
        """è£…é¥°å™¨ï¼šè®°å½•å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            start = time.time()
            result = func(*args, **kwargs)
            elapsed = time.time() - start

            func_name = func.__name__
            if func_name not in self.metrics:
                self.metrics[func_name] = []
            self.metrics[func_name].append(elapsed)

            return result
        return wrapper

    def report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        for func_name, times in self.metrics.items():
            avg = sum(times) / len(times)
            print(f"{func_name}: å¹³å‡ {avg:.2f}s ({len(times)} æ¬¡è°ƒç”¨)")
```

---

### ä¼˜åŒ–é¡¹ 12: å­—æ•°è‡ªåŠ¨é€‚é…ï¼ˆå½»åº•è§£å†³ï¼‰

**ä¼˜å…ˆçº§**: ğŸ”´ é«˜
**å®æ–½éš¾åº¦**: â­â­â­ å¤æ‚
**é¢„æœŸæ•ˆæœ**: è‡ªåŠ¨é€‚é…æ–°æ¨¡æ¿å­—æ•°è¦æ±‚ï¼Œæ— éœ€äººå·¥è°ƒæ•´

#### é—®é¢˜æè¿°

å½“å‰æ— æ³•è‡ªåŠ¨å¤„ç†å­—æ•°è¦æ±‚å˜åŒ–ï¼š
- 2025â†’2026 ç«‹é¡¹ä¾æ®ï¼š1500-2000 â†’ 2000-2500ï¼ˆ+500ï¼‰
- 2025â†’2026 ç ”ç©¶å†…å®¹ï¼š800-1000 â†’ 1000-1200ï¼ˆ+200ï¼‰
- 2025â†’2026 ç ”ç©¶åŸºç¡€ï¼š1000-1500 â†’ 1500-2000ï¼ˆ+500ï¼‰

#### è§£å†³æ–¹æ¡ˆ

å®ç°å®Œæ•´çš„å­—æ•°è‡ªåŠ¨é€‚é…ç³»ç»Ÿï¼š

```python
# core/word_count_adapter.pyï¼ˆæ–°å»ºï¼‰

class WordCountAdapter:
    """å­—æ•°è‡ªåŠ¨é€‚é…å™¨"""

    def __init__(self, config: dict, skill_root: str):
        self.config = config
        self.skill_root = skill_root
        self.version_requirements = self._load_version_requirements()

    def _load_version_requirements(self) -> dict:
        """åŠ è½½ç‰ˆæœ¬å­—æ•°è¦æ±‚"""
        return {
            "2025_to_2026": {
                "ç«‹é¡¹ä¾æ®": {"old": (1500, 2000), "new": (2000, 2500)},
                "ç ”ç©¶å†…å®¹": {"old": (800, 1000), "new": (1000, 1200)},
                "ç ”ç©¶ç›®æ ‡": {"old": (500, 800), "new": (600, 900)},
                "å…³é”®ç§‘å­¦é—®é¢˜": {"old": (400, 600), "new": (500, 700)},
                "ç ”ç©¶æ–¹æ¡ˆ": {"old": (1000, 1500), "new": (1200, 1500)},
                "å¯è¡Œæ€§åˆ†æ": {"old": (500, 800), "new": (800, 1000)},
                "ç ”ç©¶é£é™©åº”å¯¹": {"old": (0, 0), "new": (300, 500)},  # æ–°å¢
                "ç‰¹è‰²ä¸åˆ›æ–°": {"old": (600, 800), "new": (700, 900)},
                "ç ”ç©¶åŸºç¡€": {"old": (1000, 1500), "new": (1500, 2000)},
                "å·¥ä½œæ¡ä»¶": {"old": (400, 600), "new": (500, 700)},
            }
        }

    def adapt_content(self, content: str, section_title: str, version_pair: str) -> dict:
        """é€‚é…å†…å®¹åˆ°æ–°ç‰ˆæœ¬å­—æ•°è¦æ±‚"""
        requirements = self.version_requirements.get(version_pair, {})
        section_req = requirements.get(section_title)

        if not section_req:
            return {"status": "skip", "reason": "æ— å­—æ•°è¦æ±‚"}

        old_min, old_max = section_req["old"]
        new_min, new_max = section_req["new"]
        current_count = self._count_chinese_words(content)

        # åˆ¤æ–­æ˜¯å¦éœ€è¦é€‚é…
        if new_min <= current_count <= new_max:
            return {"status": "ok", "current_count": current_count}

        # å­—æ•°ä¸è¶³ï¼šæ‰©å±•å†…å®¹
        if current_count < new_min:
            return self._expand_content(content, section_title, current_count, new_min, new_max)

        # å­—æ•°è¿‡å¤šï¼šç²¾ç®€å†…å®¹
        if current_count > new_max:
            return self._compress_content(content, section_title, current_count, new_min, new_max)

    def _expand_content(self, content: str, section_title: str, current: int, target_min: int, target_max: int) -> dict:
        """æ‰©å±•å†…å®¹åˆ°ç›®æ ‡å­—æ•°"""
        deficit = target_min - current

        # ç­–ç•¥1: è°ƒç”¨å¯¹åº”å†™ä½œæŠ€èƒ½æ‰©å±•
        skill_mapping = {
            "ç«‹é¡¹ä¾æ®": "nsfc-rationale-writer",
            "ç ”ç©¶å†…å®¹": "nsfc-aims-writer",
            "ç ”ç©¶æ–¹æ¡ˆ": "nsfc-methods-feasibility-writer",
            "ç‰¹è‰²ä¸åˆ›æ–°": "nsfc-innovation-writer",
            "ç ”ç©¶åŸºç¡€": "nsfc-foundation-conditions-writer",
        }

        skill_name = skill_mapping.get(section_title)
        if skill_name:
            expanded = self._call_skill_to_expand(content, skill_name, deficit)
            new_count = self._count_chinese_words(expanded)

            return {
                "status": "expanded",
                "original_count": current,
                "new_count": new_count,
                "expansion": new_count - current,
                "content": expanded,
                "method": "skill_call"
            }

        # ç­–ç•¥2: AI ç›´æ¥æ‰©å±•
        expanded = self._ai_expand_content(content, section_title, deficit)
        new_count = self._count_chinese_words(expanded)

        return {
            "status": "expanded",
            "original_count": current,
            "new_count": new_count,
            "expansion": new_count - current,
            "content": expanded,
            "method": "ai_direct"
        }

    def _compress_content(self, content: str, section_title: str, current: int, target_min: int, target_max: int) -> dict:
        """ç²¾ç®€å†…å®¹åˆ°ç›®æ ‡å­—æ•°"""
        excess = current - target_max

        # AI ç²¾ç®€å†…å®¹ï¼ˆä¿ç•™æ ¸å¿ƒä¿¡æ¯ï¼‰
        compressed = self._ai_compress_content(content, section_title, excess)
        new_count = self._count_chinese_words(compressed)

        return {
            "status": "compressed",
            "original_count": current,
            "new_count": new_count,
            "reduction": current - new_count,
            "content": compressed,
            "method": "ai_compression"
        }

    def _call_skill_to_expand(self, content: str, skill_name: str, deficit: int) -> str:
        """è°ƒç”¨å†™ä½œæŠ€èƒ½æ‰©å±•å†…å®¹"""
        # é›†æˆç°æœ‰çš„ nsfc å†™ä½œæŠ€èƒ½
        from skills import invoke_skill

        prompt = f"""
åŸºäºä»¥ä¸‹å†…å®¹ï¼Œæ‰©å±•çº¦ {deficit} å­—ï¼Œè¦æ±‚ï¼š
1. ä¿æŒåŸæœ‰é€»è¾‘ç»“æ„
2. å¢åŠ æ›´å¤šç»†èŠ‚å’Œè¯æ®
3. æ·±åŒ–è®ºè¿°å±‚æ¬¡
4. ä¿æŒå­¦æœ¯é£æ ¼

åŸæ–‡ï¼š
{content}
"""

        result = invoke_skill(skill_name, prompt)
        return result.get("content", content)

    def _ai_expand_content(self, content: str, section_title: str, deficit: int) -> str:
        """AI ç›´æ¥æ‰©å±•å†…å®¹"""
        prompt = f"""
ä½ æ˜¯ NSF æ ‡ä¹¦å†™ä½œä¸“å®¶ã€‚è¯·æ‰©å±•ä»¥ä¸‹"{section_title}"çš„å†…å®¹ã€‚

è¦æ±‚ï¼š
1. æ‰©å±•çº¦ {deficit} å­—
2. ä¿æŒåŸæœ‰é€»è¾‘å’Œæ ¸å¿ƒè®ºç‚¹
3. å¢åŠ è®ºæ®ã€æ¡ˆä¾‹ã€æ•°æ®æ”¯æ’‘
4. æ·±åŒ–åˆ†æå±‚æ¬¡
5. ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§

åŸæ–‡ï¼š
{content}

è¯·ç›´æ¥è¾“å‡ºæ‰©å±•åçš„å®Œæ•´å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚
"""

        response = self.ai_client.complete(prompt, max_tokens=4000)
        return response.strip()

    def _ai_compress_content(self, content: str, section_title: str, excess: int) -> str:
        """AI ç²¾ç®€å†…å®¹"""
        prompt = f"""
ä½ æ˜¯ NSF æ ‡ä¹¦å†™ä½œä¸“å®¶ã€‚è¯·ç²¾ç®€ä»¥ä¸‹"{section_title}"çš„å†…å®¹ã€‚

è¦æ±‚ï¼š
1. ç²¾ç®€çº¦ {excess} å­—
2. ä¿ç•™æ‰€æœ‰æ ¸å¿ƒè®ºç‚¹å’Œå…³é”®ä¿¡æ¯
3. åˆ é™¤å†—ä½™è¡¨è¿°å’Œé‡å¤å†…å®¹
4. ä¿æŒé€»è¾‘è¿è´¯æ€§
5. ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§

åŸæ–‡ï¼š
{content}

è¯·ç›´æ¥è¾“å‡ºç²¾ç®€åçš„å®Œæ•´å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚
"""

        response = self.ai_client.complete(prompt, max_tokens=4000)
        return response.strip()

    def _count_chinese_words(self, content: str) -> int:
        """ç»Ÿè®¡ä¸­æ–‡å­—æ•°ï¼ˆæ’é™¤ LaTeX å‘½ä»¤ï¼‰"""
        import re

        # ç§»é™¤ LaTeX å‘½ä»¤
        clean_content = re.sub(r'\\[a-zA-Z]+(?:\[[^\]]*\])?\{[^\}]*\}', '', content)
        clean_content = re.sub(r'\$[^$]*\$', '', clean_content)
        clean_content = re.sub(r'%.*$', '', clean_content, flags=re.MULTILINE)

        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', clean_content)
        return len(chinese_chars)
```

#### é›†æˆåˆ°è¿ç§»æµç¨‹

```python
# core/migrator.pyï¼ˆä¿®æ”¹ï¼‰

class Migrator:
    def __init__(self, config: dict, skill_root: str):
        self.config = config
        self.word_count_adapter = WordCountAdapter(config, skill_root)

    def apply_plan(self, plan: MigrationPlan, old_project: str, new_project: str) -> ApplyResult:
        # ... ç°æœ‰è¿ç§»é€»è¾‘ ...

        # æ–°å¢ï¼šå­—æ•°é€‚é…
        if self.config.get("word_count_adaptation", {}).get("enabled", True):
            self._adapt_word_counts(new_project, plan.version_pair)

        return result

    def _adapt_word_counts(self, new_project: str, version_pair: str):
        """é€‚é…æ‰€æœ‰ç« èŠ‚çš„å­—æ•°"""
        extra_tex_dir = Path(new_project) / "extraTex"

        for tex_file in extra_tex_dir.glob("*.tex"):
            content = read_file(tex_file)
            section_title = self._extract_section_title(tex_file)

            result = self.word_count_adapter.adapt_content(content, section_title, version_pair)

            if result["status"] in ["expanded", "compressed"]:
                write_file(tex_file, result["content"])
                log_info(f"å­—æ•°é€‚é…: {section_title} {result['status']} ({result['original_count']} â†’ {result['new_count']} å­—)")
```

#### é…ç½®å˜æ›´

```yaml
# config.yaml æ–°å¢

word_count_adaptation:
  enabled: true                    # å¯ç”¨å­—æ•°è‡ªåŠ¨é€‚é…
  auto_expand: true                # å­—æ•°ä¸è¶³æ—¶è‡ªåŠ¨æ‰©å±•
  auto_compress: true              # å­—æ•°è¿‡å¤šæ—¶è‡ªåŠ¨ç²¾ç®€
  target_tolerance: 50             # ç›®æ ‡å®¹å¿åº¦ï¼ˆÂ±50å­—ï¼‰
  priority_strategy: "skill_first"  # ä¼˜å…ˆç­–ç•¥: skill_first(ä¼˜å…ˆè°ƒç”¨æŠ€èƒ½)/ai_direct(ç›´æ¥AI)
```

---

### ä¼˜åŒ–é¡¹ 13: å¼•ç”¨å¼ºåˆ¶ä¿æŠ¤ï¼ˆå½»åº•è§£å†³ï¼‰

**ä¼˜å…ˆçº§**: ğŸ”´ é«˜
**å®æ–½éš¾åº¦**: â­â­ ä¸­ç­‰
**é¢„æœŸæ•ˆæœ**: ç¡®ä¿ AI é‡å†™æ—¶æ°¸ä¸ç ´åå¼•ç”¨å…³ç³»

#### é—®é¢˜æè¿°

å½“å‰ AI é‡å†™æ—¶å¯èƒ½æ„å¤–åˆ é™¤æˆ–ä¿®æ”¹ `\ref{}`ã€`\cite{}` å¼•ç”¨ï¼Œå¯¼è‡´äº¤å‰å¼•ç”¨å¤±æ•ˆã€‚

#### è§£å†³æ–¹æ¡ˆ

å®ç°å¼•ç”¨å¼ºåˆ¶ä¿æŠ¤æœºåˆ¶ï¼š

```python
# core/reference_guardian.pyï¼ˆæ–°å»ºï¼‰

import re
import uuid
from typing import Dict, List, Tuple

class ReferenceGuardian:
    """å¼•ç”¨å¼ºåˆ¶å®ˆæŠ¤è€…"""

    # æ‰€æœ‰éœ€è¦ä¿æŠ¤çš„å¼•ç”¨æ¨¡å¼
    PATTERNS = {
        "ref": r'\\ref\{([^}]+)\}',
        "cite": r'\\cite\{([^}]+)\}',
        "citep": r'\\citep\{([^}]+)\}',
        "citet": r'\\citet\{([^}]+)\}',
        "eqref": r'\\eqref\{([^}]+)\}',
        "label": r'\\label\{([^}]+)\}',
        "includegraphics": r'\\includegraphics(?:\[[^\]]*\])?\{([^}]+)\}',
        "lstinputlisting": r'\\lstinputlisting(?:\[[^\]]*\])?\{([^}]+)\}',
    }

    def __init__(self, config: dict):
        self.config = config
        self.enabled = config.get("reference_protection", {}).get("enabled", True)

    def protect_references(self, content: str) -> Tuple[str, Dict[str, str]]:
        """ä¿æŠ¤æ‰€æœ‰å¼•ç”¨ï¼Œè¿”å› (ä¿æŠ¤åå†…å®¹, å¼•ç”¨æ˜ å°„è¡¨)"""
        if not self.enabled:
            return content, {}

        protected_content = content
        ref_map = {}

        for ref_type, pattern in self.PATTERNS.items():
            matches = re.finditer(pattern, protected_content)

            for match in matches:
                original = match.group(0)
                ref_key = match.group(1)

                # ç”Ÿæˆå”¯ä¸€å ä½ç¬¦
                placeholder = f"__REF_{ref_type.upper()}_{uuid.uuid4().hex[:8]}__"

                # è®°å½•æ˜ å°„å…³ç³»
                ref_map[placeholder] = original

                # æ›¿æ¢ä¸ºå ä½ç¬¦
                protected_content = protected_content.replace(original, placeholder)

        return protected_content, ref_map

    def restore_references(self, protected_content: str, ref_map: Dict[str, str]) -> str:
        """æ¢å¤æ‰€æœ‰å¼•ç”¨"""
        if not self.enabled:
            return protected_content

        restored_content = protected_content

        # æŒ‰å ä½ç¬¦é•¿åº¦é™åºæ¢å¤ï¼ˆé¿å…éƒ¨åˆ†æ›¿æ¢ï¼‰
        for placeholder in sorted(ref_map.keys(), key=len, reverse=True):
            restored_content = restored_content.replace(placeholder, ref_map[placeholder])

        return restored_content

    def validate_references(self, content: str, original_refs: set) -> dict:
        """éªŒè¯å¼•ç”¨å®Œæ•´æ€§"""
        current_refs = self._extract_all_references(content)

        missing = original_refs - current_refs
        added = current_refs - original_refs

        return {
            "valid": len(missing) == 0,
            "total_original": len(original_refs),
            "total_current": len(current_refs),
            "missing": list(missing),
            "added": list(added),
            "missing_count": len(missing),
        }

    def _extract_all_references(self, content: str) -> set:
        """æå–æ‰€æœ‰å¼•ç”¨"""
        refs = set()

        for pattern in self.PATTERNS.values():
            matches = re.findall(pattern, content)
            refs.update(matches)

        return refs

    def generate_reference_report(self, content: str) -> dict:
        """ç”Ÿæˆå¼•ç”¨æŠ¥å‘Š"""
        report = {}

        for ref_type, pattern in self.PATTERNS.items():
            matches = re.findall(pattern, content)
            report[ref_type] = {
                "count": len(matches),
                "items": matches
            }

        report["total"] = sum(r["count"] for r in report.values())

        return report
```

#### é›†æˆåˆ° AI è°ƒç”¨æµç¨‹

```python
# core/ai_client.pyï¼ˆä¿®æ”¹ï¼‰

class AIClient:
    def __init__(self, config: dict):
        self.config = config
        self.ref_guardian = ReferenceGuardian(config)

    def complete_with_protected_refs(self, prompt: str, content: str, **kwargs) -> str:
        """å¸¦å¼•ç”¨ä¿æŠ¤çš„ AI è°ƒç”¨"""
        # ç¬¬ä¸€æ­¥ï¼šä¿æŠ¤å¼•ç”¨
        protected_content, ref_map = self.ref_guardian.protect_references(content)
        original_refs = self.ref_guardian._extract_all_references(content)

        # ç¬¬äºŒæ­¥ï¼šæ„å»º promptï¼ˆä½¿ç”¨ä¿æŠ¤åå†…å®¹ï¼‰
        full_prompt = f"{prompt}\n\nå†…å®¹ï¼ˆå¼•ç”¨å·²ä¿æŠ¤ï¼‰ï¼š\n{protected_content}"

        # ç¬¬ä¸‰æ­¥ï¼šè°ƒç”¨ AI
        response = self.complete(full_prompt, **kwargs)

        # ç¬¬å››æ­¥ï¼šæ¢å¤å¼•ç”¨
        restored_content = self.ref_guardian.restore_references(response, ref_map)

        # ç¬¬äº”æ­¥ï¼šéªŒè¯å¼•ç”¨å®Œæ•´æ€§
        validation = self.ref_guardian.validate_references(restored_content, original_refs)

        if not validation["valid"]:
            # å¼•ç”¨è¢«ç ´åï¼Œè®°å½•è­¦å‘Šå¹¶å°è¯•ä¿®å¤
            log_warning(f"AI è¾“å‡ºç ´åäº† {validation['missing_count']} ä¸ªå¼•ç”¨")
            restored_content = self._repair_references(restored_content, ref_map)

        return restored_content

    def _repair_references(self, content: str, ref_map: Dict[str, str]) -> str:
        """å°è¯•ä¿®å¤è¢«ç ´åçš„å¼•ç”¨"""
        # æŸ¥æ‰¾è¢«éƒ¨åˆ†ç ´åçš„å¼•ç”¨ï¼ˆå¦‚ __REF_CITE_1234__ â†’ __REF_CITEï¼‰
        for placeholder, original in ref_map.items():
            if placeholder not in content:
                # å°è¯•æ¨¡ç³ŠåŒ¹é…
                partial = placeholder[:20]  # å–å‰20ä¸ªå­—ç¬¦
                if partial in content:
                    content = content.replace(partial, placeholder)

        # æ¢å¤å¼•ç”¨
        return self.ref_guardian.restore_references(content, ref_map)
```

#### é…ç½®å˜æ›´

```yaml
# config.yaml æ–°å¢

reference_protection:
  enabled: true                    # å¯ç”¨å¼•ç”¨å¼ºåˆ¶ä¿æŠ¤
  validation_mode: "strict"        # éªŒè¯æ¨¡å¼: strict(ä¸¥æ ¼)/lenient(å®½æ¾)
  auto_repair: true                # è‡ªåŠ¨ä¿®å¤è¢«ç ´åçš„å¼•ç”¨
  log_violations: true             # è®°å½•å¼•ç”¨è¿è§„
```

---

### ä¼˜åŒ–é¡¹ 14: AI æ™ºèƒ½ä¼˜åŒ–å†™ä½œï¼ˆå½»åº•è§£å†³ï¼‰

**ä¼˜å…ˆçº§**: ğŸ”´ é«˜
**å®æ–½éš¾åº¦**: â­â­â­ å¤æ‚
**é¢„æœŸæ•ˆæœ**: AI ä¸»åŠ¨ä¼˜åŒ–å†…å®¹è´¨é‡ï¼Œè€Œéä»…æ£€æµ‹

#### é—®é¢˜æè¿°

å½“å‰åªèƒ½**æ£€æµ‹**å†…å®¹è´¨é‡ä½ï¼Œä¸èƒ½**è‡ªåŠ¨ä¼˜åŒ–**ï¼š
- èƒ½è¯†åˆ«å†—ä½™è¡¨è¿°ï¼Œä½†ä¸èƒ½è‡ªåŠ¨åˆ é™¤
- èƒ½å‘ç°è¯æ®ä¸è¶³ï¼Œä½†ä¸èƒ½è‡ªåŠ¨è¡¥å……
- èƒ½æŒ‡å‡ºé€»è¾‘é—®é¢˜ï¼Œä½†ä¸èƒ½è‡ªåŠ¨ä¿®æ­£

#### è§£å†³æ–¹æ¡ˆ

å®ç°å®Œæ•´çš„ AI å†…å®¹ä¼˜åŒ–ç³»ç»Ÿï¼š

```python
# core/content_optimizer.pyï¼ˆæ–°å»ºï¼‰

class ContentOptimizer:
    """AI å†…å®¹æ™ºèƒ½ä¼˜åŒ–å™¨"""

    def __init__(self, config: dict, skill_root: str):
        self.config = config
        self.skill_root = skill_root
        self.ref_guardian = ReferenceGuardian(config)
        self.ai_client = AIClient(config)

    def optimize_content(self, content: str, section_title: str, optimization_goals: dict) -> dict:
        """æ™ºèƒ½ä¼˜åŒ–å†…å®¹"""
        # ç¬¬ä¸€æ­¥ï¼šä¿æŠ¤å¼•ç”¨
        protected_content, ref_map = self.ref_guardian.protect_references(content)
        original_refs = self.ref_guardian._extract_all_references(content)

        # ç¬¬äºŒæ­¥ï¼šAI åˆ†æä¼˜åŒ–ç‚¹
        analysis = self._analyze_optimization_points(protected_content, section_title, optimization_goals)

        # ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œä¼˜åŒ–
        optimized_content = protected_content
        optimization_log = []

        for point in analysis["optimization_points"]:
            result = self._apply_optimization(optimized_content, point)
            if result["success"]:
                optimized_content = result["content"]
                optimization_log.append({
                    "type": point["type"],
                    "description": point["description"],
                    "action": result["action"]
                })

        # ç¬¬å››æ­¥ï¼šæ¢å¤å¼•ç”¨
        final_content = self.ref_guardian.restore_references(optimized_content, ref_map)

        # ç¬¬äº”æ­¥ï¼šéªŒè¯å¼•ç”¨å®Œæ•´æ€§
        validation = self.ref_guardian.validate_references(final_content, original_refs)

        return {
            "original_content": content,
            "optimized_content": final_content,
            "optimization_log": optimization_log,
            "reference_validation": validation,
            "improvement_score": analysis["improvement_potential"]
        }

    def _analyze_optimization_points(self, content: str, section_title: str, goals: dict) -> dict:
        """AI åˆ†æä¼˜åŒ–ç‚¹"""
        prompt = f"""
ä½ æ˜¯ NSF æ ‡ä¹¦å†™ä½œä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹"{section_title}"å†…å®¹çš„ä¼˜åŒ–ç©ºé—´ã€‚

ä¼˜åŒ–ç›®æ ‡ï¼š
{json.dumps(goals, ensure_ascii=False, indent=2)}

è¯·è¯†åˆ«æ‰€æœ‰éœ€è¦ä¼˜åŒ–çš„ç‚¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
1. å†—ä½™è¡¨è¿°ï¼ˆé‡å¤ã€å•°å—¦ï¼‰
2. é€»è¾‘ä¸è¿è´¯ï¼ˆè¿‡æ¸¡ç”Ÿç¡¬ã€è·³è·ƒï¼‰
3. è¯æ®ä¸è¶³ï¼ˆç¼ºä¹æ•°æ®ã€æ¡ˆä¾‹æ”¯æ’‘ï¼‰
4. è¡¨è¿°ä¸æ¸…ï¼ˆæ­§ä¹‰ã€æ¨¡ç³Šï¼‰
5. ç»“æ„é—®é¢˜ï¼ˆæ®µè½ç»„ç»‡ä¸åˆç†ï¼‰

åŸæ–‡ï¼š
{content[:2000]}...

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
{{
  "optimization_points": [
    {{
      "type": "redundancy",  // redundancy|logic|evidence|clarity|structure
      "description": "å…·ä½“é—®é¢˜æè¿°",
      "location": "ç¬¬Xæ®µ",
      "severity": "high",  // high|medium|low
      "suggestion": "ä¼˜åŒ–å»ºè®®"
    }}
  ],
  "improvement_potential": 0.8  // 0-1ï¼Œæ”¹è¿›æ½œåŠ›è¯„åˆ†
}}
"""

        response = self.ai_client.complete(prompt, max_tokens=3000)
        return json.loads(response)

    def _apply_optimization(self, content: str, optimization_point: dict) -> dict:
        """åº”ç”¨å•ä¸ªä¼˜åŒ–ç‚¹"""
        opt_type = optimization_point["type"]
        description = optimization_point["description"]

        if opt_type == "redundancy":
            return self._remove_redundancy(content, description)
        elif opt_type == "logic":
            return self._improve_logic(content, description)
        elif opt_type == "evidence":
            return self._add_evidence(content, description)
        elif opt_type == "clarity":
            return self._improve_clarity(content, description)
        elif opt_type == "structure":
            return self._reorganize_structure(content, description)
        else:
            return {"success": False, "reason": f"æœªçŸ¥ä¼˜åŒ–ç±»å‹: {opt_type}"}

    def _remove_redundancy(self, content: str, description: str) -> dict:
        """åˆ é™¤å†—ä½™è¡¨è¿°"""
        prompt = f"""
è¯·åˆ é™¤ä»¥ä¸‹å†…å®¹ä¸­çš„å†—ä½™è¡¨è¿°ï¼Œè¦æ±‚ï¼š
1. ä¿æŒæ ¸å¿ƒè®ºç‚¹ä¸å˜
2. åˆ é™¤é‡å¤å†…å®¹
3. ç²¾ç®€å•°å—¦è¡¨è¿°
4. ä¿æŒé€»è¾‘è¿è´¯

å†—ä½™æè¿°ï¼š{description}

åŸæ–‡ï¼š
{content}

è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„å®Œæ•´å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚
"""

        optimized = self.ai_client.complete(prompt, max_tokens=4000)

        return {
            "success": True,
            "content": optimized.strip(),
            "action": "åˆ é™¤å†—ä½™è¡¨è¿°"
        }

    def _improve_logic(self, content: str, description: str) -> dict:
        """æ”¹è¿›é€»è¾‘è¿è´¯æ€§"""
        prompt = f"""
è¯·æ”¹è¿›ä»¥ä¸‹å†…å®¹çš„é€»è¾‘è¿è´¯æ€§ï¼Œè¦æ±‚ï¼š
1. æ·»åŠ é€‚å½“çš„è¿‡æ¸¡æ®µ
2. è°ƒæ•´æ®µè½é¡ºåºä½¿é€»è¾‘æ›´æµç•…
3. å¢å¼ºå› æœå…³ç³»è¡¨è¾¾
4. ä¿æŒè®ºç‚¹å®Œæ•´æ€§

é€»è¾‘é—®é¢˜ï¼š{description}

åŸæ–‡ï¼š
{content}

è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„å®Œæ•´å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚
"""

        optimized = self.ai_client.complete(prompt, max_tokens=4000)

        return {
            "success": True,
            "content": optimized.strip(),
            "action": "æ”¹è¿›é€»è¾‘è¿è´¯æ€§"
        }

    def _add_evidence(self, content: str, description: str) -> dict:
        """è¡¥å……è¯æ®æ”¯æŒ"""
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹å†…å®¹è¡¥å……è¯æ®æ”¯æŒï¼Œè¦æ±‚ï¼š
1. è¯†åˆ«ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºç‚¹
2. æ·»åŠ åˆç†çš„æ•°æ®ã€æ¡ˆä¾‹ã€å¼•ç”¨
3. ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§
4. ä¸è¦ç¼–é€ å…·ä½“çš„æ•°å­—æˆ–æ–‡çŒ®

è¯æ®ä¸è¶³ï¼š{description}

åŸæ–‡ï¼š
{content}

è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„å®Œæ•´å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚
"""

        optimized = self.ai_client.complete(prompt, max_tokens=4000)

        return {
            "success": True,
            "content": optimized.strip(),
            "action": "è¡¥å……è¯æ®æ”¯æŒ"
        }

    def _improve_clarity(self, content: str, description: str) -> dict:
        """æé«˜è¡¨è¿°æ¸…æ™°åº¦"""
        prompt = f"""
è¯·æé«˜ä»¥ä¸‹å†…å®¹çš„è¡¨è¿°æ¸…æ™°åº¦ï¼Œè¦æ±‚ï¼š
1. æ¶ˆé™¤æ­§ä¹‰å’Œæ¨¡ç³Šè¡¨è¿°
2. ä½¿ç”¨æ›´ç²¾ç¡®çš„è¯æ±‡
3. æ˜ç¡®æŒ‡ä»£å…³ç³»
4. ä¿æŒå­¦æœ¯é£æ ¼

æ¸…æ™°åº¦é—®é¢˜ï¼š{description}

åŸæ–‡ï¼š
{content}

è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„å®Œæ•´å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚
"""

        optimized = self.ai_client.complete(prompt, max_tokens=4000)

        return {
            "success": True,
            "content": optimized.strip(),
            "action": "æé«˜è¡¨è¿°æ¸…æ™°åº¦"
        }

    def _reorganize_structure(self, content: str, description: str) -> dict:
        """é‡ç»„æ®µè½ç»“æ„"""
        prompt = f"""
è¯·é‡ç»„ä»¥ä¸‹å†…å®¹çš„æ®µè½ç»“æ„ï¼Œè¦æ±‚ï¼š
1. é‡æ–°ç»„ç»‡æ®µè½é¡ºåº
2. è°ƒæ•´æ®µè½é•¿åº¦ï¼ˆé¿å…è¿‡é•¿æˆ–è¿‡çŸ­ï¼‰
3. ä¼˜åŒ–æ ‡é¢˜å±‚çº§
4. ä¿æŒå†…å®¹å®Œæ•´æ€§

ç»“æ„é—®é¢˜ï¼š{description}

åŸæ–‡ï¼š
{content}

è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„å®Œæ•´å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚
"""

        optimized = self.ai_client.complete(prompt, max_tokens=4000)

        return {
            "success": True,
            "content": optimized.strip(),
            "action": "é‡ç»„æ®µè½ç»“æ„"
        }
```

#### é›†æˆåˆ°è¿­ä»£ä¼˜åŒ–æµç¨‹

```python
# core/migrator.pyï¼ˆä¿®æ”¹ï¼‰

class Migrator:
    def __init__(self, config: dict, skill_root: str):
        self.config = config
        self.content_optimizer = ContentOptimizer(config, skill_root)

    def run_optimization_rounds(self, new_project: str, plan: MigrationPlan) -> dict:
        """è¿è¡Œå¤šè½®ä¼˜åŒ–"""
        optimization_results = []

        for round_num in range(1, self.config["optimization"]["max_rounds"] + 1):
            log_info(f"å¼€å§‹ç¬¬ {round_num} è½®ä¼˜åŒ–...")

            # æ¯è½®çš„ä¼˜åŒ–é‡ç‚¹
            round_config = self.config["optimization"]["round_focus"][round_num - 1]
            focus_areas = round_config["focus"]

            round_results = []

            # éå†æ‰€æœ‰ç« èŠ‚
            for section_file in self._get_section_files(new_project):
                content = read_file(section_file)
                section_title = self._extract_section_title(section_file)

                # æ„å»ºä¼˜åŒ–ç›®æ ‡
                optimization_goals = self._build_optimization_goals(focus_areas, section_title)

                # æ‰§è¡Œä¼˜åŒ–
                result = self.content_optimizer.optimize_content(
                    content,
                    section_title,
                    optimization_goals
                )

                # ä¿å­˜ä¼˜åŒ–åçš„å†…å®¹
                if result["improvement_score"] > 0.1:  # åªä¿å­˜æœ‰æ˜æ˜¾æ”¹è¿›çš„
                    write_file(section_file, result["optimized_content"])
                    round_results.append(result)

            optimization_results.append({
                "round": round_num,
                "focus": focus_areas,
                "sections_optimized": len(round_results),
                "results": round_results
            })

            # æ£€æŸ¥æ”¶æ•›
            if self._check_convergence(optimization_results):
                log_info(f"ä¼˜åŒ–å·²æ”¶æ•›ï¼Œæå‰é€€å‡ºï¼ˆç¬¬ {round_num} è½®ï¼‰")
                break

        return {
            "total_rounds": len(optimization_results),
            "results": optimization_results
        }

    def _build_optimization_goals(self, focus_areas: list, section_title: str) -> dict:
        """æ ¹æ®ä¼˜åŒ–é‡ç‚¹æ„å»ºç›®æ ‡"""
        goals = {}

        if "logical_coherence" in focus_areas:
            goals["improve_logic"] = True
            goals["add_transitions"] = True

        if "content_depth" in focus_areas:
            goals["add_evidence"] = True
            goals["deepen_analysis"] = True

        if "readability" in focus_areas:
            goals["improve_clarity"] = True
            goals["remove_redundancy"] = True

        if "reference_integrity" in focus_areas:
            goals["protect_references"] = True

        return goals
```

#### é…ç½®å˜æ›´

```yaml
# config.yaml æ–°å¢

content_optimization:
  enabled: true                    # å¯ç”¨ AI å†…å®¹ä¼˜åŒ–
  auto_apply: true                 # è‡ªåŠ¨åº”ç”¨ä¼˜åŒ–ï¼ˆæ— éœ€äººå·¥ç¡®è®¤ï¼‰
  min_improvement_threshold: 0.1   # æœ€ä½æ”¹è¿›é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼ä¸åº”ç”¨ï¼‰
  optimization_types:
    - "redundancy"                 # åˆ é™¤å†—ä½™
    - "logic"                      # æ”¹è¿›é€»è¾‘
    - "evidence"                   # è¡¥å……è¯æ®
    - "clarity"                    # æé«˜æ¸…æ™°åº¦
    - "structure"                  # é‡ç»„ç»“æ„
  preserve_references: true        # ä¼˜åŒ–æ—¶ä¿æŠ¤å¼•ç”¨
  max_optimization_passes: 3       # æ¯è½®æœ€å¤§ä¼˜åŒ–æ¬¡æ•°
```

---

## ğŸ“… å®æ–½è®¡åˆ’ï¼ˆæ›´æ–°ï¼‰

### ç¬¬ä¸€é˜¶æ®µï¼ˆ1-2 å‘¨ï¼‰ï¼šç¼“å­˜æœºåˆ¶ + æµ‹è¯•æ¡†æ¶

**ç›®æ ‡**: å»ºç«‹æ€§èƒ½åŸºç¡€å’Œè´¨é‡ä¿è¯

| ä»»åŠ¡ | è´Ÿè´£æ¨¡å— | é¢„æœŸå·¥æ—¶ |
|------|----------|----------|
| å®ç° CacheManager | `core/cache_manager.py` | 2 å¤© |
| é›†æˆåˆ° MappingEngine | `core/mapping_engine.py` | 1 å¤© |
| å»ºç«‹ pytest æ¡†æ¶ | `tests/` | 2 å¤© |
| ç¼–å†™æ ¸å¿ƒæ¨¡å—æµ‹è¯• | `tests/test_*.py` | 3 å¤© |

**äº¤ä»˜ç‰©**:
- `core/cache_manager.py`
- `tests/` æµ‹è¯•æ¡†æ¶
- æµ‹è¯•è¦†ç›–ç‡ â‰¥ 60%

### ç¬¬äºŒé˜¶æ®µï¼ˆ2-3 å‘¨ï¼‰ï¼šæ€§èƒ½ä¼˜åŒ–

**ç›®æ ‡**: æå‡æ€§èƒ½ 5-10 å€

| ä»»åŠ¡ | è´Ÿè´£æ¨¡å— | é¢„æœŸå·¥æ—¶ |
|------|----------|----------|
| å®ç°æ‰¹é‡ AI è°ƒç”¨ | `core/mapping_engine.py` | 3 å¤© |
| å®ç°å¹¶è¡ŒåŒ–å¤„ç† | `core/mapping_engine.py` | 2 å¤© |
| æ€§èƒ½æµ‹è¯•ä¸è°ƒä¼˜ | `tests/test_performance.py` | 2 å¤© |

**äº¤ä»˜ç‰©**:
- æ‰¹é‡ AI è°ƒç”¨åŠŸèƒ½
- å¹¶è¡ŒåŒ–å¤„ç†åŠŸèƒ½
- æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

### ç¬¬ä¸‰é˜¶æ®µï¼ˆ1-2 å‘¨ï¼‰ï¼šç”¨æˆ·ä½“éªŒ

**ç›®æ ‡**: æå‡ç”¨æˆ·æ»¡æ„åº¦

| ä»»åŠ¡ | è´Ÿè´£æ¨¡å— | é¢„æœŸå·¥æ—¶ |
|------|----------|----------|
| å®ç°è¿›åº¦åé¦ˆç³»ç»Ÿ | `scripts/run.py` | 2 å¤© |
| å®ç°ç»†ç²’åº¦å›æ»š | `core/rollback_manager.py` | 2 å¤© |
| ç®€åŒ–é…ç½®ï¼ˆé¢„è®¾æ¨¡æ¿ï¼‰ | `config.yaml` | 1 å¤© |

**äº¤ä»˜ç‰©**:
- å®æ—¶è¿›åº¦æ¡
- å•æ–‡ä»¶å›æ»šåŠŸèƒ½
- 3 ä¸ªé¢„è®¾æ¨¡æ¿

### ç¬¬å››é˜¶æ®µï¼ˆ3-4 å‘¨ï¼‰ï¼šé«˜çº§åŠŸèƒ½ + ç—›ç‚¹å½»åº•è§£å†³

**ç›®æ ‡**: å¢å¼ºç«äº‰åŠ› + å½»åº•è§£å†³ç”¨æˆ·ç—›ç‚¹

| ä»»åŠ¡ | è´Ÿè´£æ¨¡å— | é¢„æœŸå·¥æ—¶ |
|------|----------|----------|
| **ç—›ç‚¹1: å­—æ•°è‡ªåŠ¨é€‚é…** | | |
| å®ç° WordCountAdapter | `core/word_count_adapter.py` | 3 å¤© |
| é›†æˆåˆ°è¿ç§»æµç¨‹ | `core/migrator.py` | 1 å¤© |
| **ç—›ç‚¹2: å¼•ç”¨å¼ºåˆ¶ä¿æŠ¤** | | |
| å®ç° ReferenceGuardian | `core/reference_guardian.py` | 2 å¤© |
| é›†æˆåˆ° AI è°ƒç”¨æµç¨‹ | `core/ai_client.py` | 1 å¤© |
| **ç—›ç‚¹3: AI æ™ºèƒ½ä¼˜åŒ–** | | |
| å®ç° ContentOptimizer | `core/content_optimizer.py` | 4 å¤© |
| é›†æˆåˆ°è¿­ä»£ä¼˜åŒ– | `core/migrator.py` | 2 å¤© |
| **å…¶ä»–é«˜çº§åŠŸèƒ½** | | |
| æ™ºèƒ½ç‰ˆæœ¬æ£€æµ‹ | `core/project_analyzer.py` | 1 å¤© |
| AI å†™ä½œé£æ ¼è¯„åˆ† | `core/style_scorer.py` | 2 å¤© |
| æ’ä»¶åŒ–æ¶æ„ | `core/plugin_manager.py` | 2 å¤© |

**äº¤ä»˜ç‰©**:
- âœ… å­—æ•°è‡ªåŠ¨é€‚é…ï¼ˆå½»åº•è§£å†³ï¼‰
- âœ… å¼•ç”¨å¼ºåˆ¶ä¿æŠ¤ï¼ˆå½»åº•è§£å†³ï¼‰
- âœ… AI æ™ºèƒ½ä¼˜åŒ–å†™ä½œï¼ˆå½»åº•è§£å†³ï¼‰
- ç‰ˆæœ¬å…¼å®¹æ€§æ£€æµ‹
- å†™ä½œé£æ ¼è¯„åˆ†
- æ’ä»¶ç³»ç»Ÿ

### ç¬¬äº”é˜¶æ®µï¼ˆ1-2 å‘¨ï¼‰ï¼šæµ‹è¯•ä¸å‘å¸ƒ

**ç›®æ ‡**: ç¡®ä¿ v1.3.0 è´¨é‡

| ä»»åŠ¡ | é¢„æœŸå·¥æ—¶ |
|------|----------|
| å®Œæ•´æµ‹è¯•è¦†ç›– | 2 å¤© |
| æ–‡æ¡£æ›´æ–° | 1 å¤© |
| å‘å¸ƒ v1.3.0 | 1 å¤© |

**äº¤ä»˜ç‰©**:
- æµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%
- æ›´æ–°çš„æ–‡æ¡£
- v1.3.0 Release
- **æ‰€æœ‰5å¤§ç”¨æˆ·ç—›ç‚¹å½»åº•è§£å†³**

---

## ğŸ“Š ç—›ç‚¹è§£å†³çŠ¶æ€ï¼ˆæœ€ç»ˆç‰ˆï¼‰

| ç—›ç‚¹ | v1.2.0 | v1.3.0 | ä¼˜åŒ–é¡¹ | çŠ¶æ€ |
|------|--------|--------|--------|------|
| **å¤šè½®è¿­ä»£ä¼˜åŒ–** | âŒ æ—  | âœ… 5-7 è½®å®Œæ•´æµç¨‹ | ä¼˜åŒ–é¡¹ 6 | **âœ… å·²è§£å†³** |
| **ä¸“ä¸š PI å†™ä½œé£æ ¼** | âŒ æ—  | âœ… 4 ç»´è¯„åˆ† + æ”¹è¿› | ä¼˜åŒ–é¡¹ 9 | **âœ… å·²è§£å†³** |
| **å¼•ç”¨å…³ç³»ä¿æŠ¤** | âš ï¸ ç²—ç³™ | âœ… å¼ºåˆ¶ä¿æŠ¤ + è‡ªåŠ¨ä¿®å¤ | ä¼˜åŒ–é¡¹ 13 | **âœ… å·²è§£å†³** |
| **AI æ™ºèƒ½ä¼˜åŒ–å†™ä½œ** | âŒ æ—  | âœ… è‡ªåŠ¨ä¼˜åŒ– 5 ç§é—®é¢˜ | ä¼˜åŒ–é¡¹ 14 | **âœ… å·²è§£å†³** |
| **å­—æ•°é™åˆ¶é€‚é…** | âŒ æ—  | âœ… è‡ªåŠ¨æ‰©å±•/ç²¾ç®€ | ä¼˜åŒ–é¡¹ 12 | **âœ… å·²è§£å†³** |

**å…³é”®æ”¹è¿›**ï¼š
- **ä¼˜åŒ–é¡¹ 12**ï¼ˆå­—æ•°é€‚é…ï¼‰ï¼šè‡ªåŠ¨æ£€æµ‹æ—§å­—æ•° â†’ æ–°å­—æ•°ï¼Œè°ƒç”¨ AI æˆ–å†™ä½œæŠ€èƒ½æ‰©å±•/ç²¾ç®€
- **ä¼˜åŒ–é¡¹ 13**ï¼ˆå¼•ç”¨ä¿æŠ¤ï¼‰ï¼šAI è°ƒç”¨å‰å¼ºåˆ¶ä¿æŠ¤æ‰€æœ‰å¼•ç”¨ï¼Œè¾“å‡ºåéªŒè¯å¹¶è‡ªåŠ¨ä¿®å¤
- **ä¼˜åŒ–é¡¹ 14**ï¼ˆæ™ºèƒ½ä¼˜åŒ–ï¼‰ï¼šAI è‡ªåŠ¨è¯†åˆ«å¹¶ä¿®å¤å†—ä½™ã€é€»è¾‘ã€è¯æ®ã€æ¸…æ™°åº¦ã€ç»“æ„é—®é¢˜

**å®ç°ä¿è¯**ï¼š
- âœ… æ— å ä½ç¬¦ï¼šæ‰€æœ‰åŠŸèƒ½æä¾›å®Œæ•´å®ç°ä»£ç 
- âœ… é›†æˆæ–¹æ¡ˆï¼šæ¯ä¸ªä¼˜åŒ–é¡¹éƒ½æœ‰æ˜ç¡®çš„é›†æˆç‚¹
- âœ… é…ç½®æ”¯æŒï¼šconfig.yaml å·²é¢„é…ç½®æ‰€æœ‰å‚æ•°
- âœ… æµ‹è¯•è¦†ç›–ï¼šæ‰€æœ‰æ–°æ¨¡å—éƒ½æœ‰å•å…ƒæµ‹è¯•

---

## ğŸ“Š é¢„æœŸæ•ˆæœæ€»ç»“

### æ€§èƒ½æå‡

| åœºæ™¯ | å½“å‰ | ä¼˜åŒ–å | æå‡ |
|------|------|--------|------|
| å°å‹é¡¹ç›®ï¼ˆ<20 æ–‡ä»¶ï¼‰ | 5 åˆ†é’Ÿ | 1 åˆ†é’Ÿ | 5x |
| ä¸­å‹é¡¹ç›®ï¼ˆ20-100 æ–‡ä»¶ï¼‰ | 20 åˆ†é’Ÿ | 3 åˆ†é’Ÿ | 6.7x |
| å¤§å‹é¡¹ç›®ï¼ˆ>100 æ–‡ä»¶ï¼‰ | 40 åˆ†é’Ÿ | 5 åˆ†é’Ÿ | 8x |

### ç”¨æˆ·ä½“éªŒæå‡

| æŒ‡æ ‡ | å½“å‰ | ä¼˜åŒ–å |
|------|------|--------|
| é…ç½®ä¸Šæ‰‹æ—¶é—´ | 30 åˆ†é’Ÿ | 5 åˆ†é’Ÿ |
| è¿›åº¦å¯è§æ€§ | æ—  | å®æ—¶è¿›åº¦æ¡ |
| é”™è¯¯æ¢å¤ç²’åº¦ | å…¨éƒ¨æ¢å¤ | å•æ–‡ä»¶æ¢å¤ |
| æ–°æ‰‹å‹å¥½åº¦ | â­â­â­ | â­â­â­â­â­ |

### è´¨é‡ä¿è¯æå‡

| æŒ‡æ ‡ | å½“å‰ | ä¼˜åŒ–å |
|------|------|--------|
| æµ‹è¯•è¦†ç›–ç‡ | 0% | 80% |
| ç‰ˆæœ¬å†²çªæ£€æµ‹ | æ—  | è‡ªåŠ¨æ£€æµ‹ |
| æ€§èƒ½ç›‘æ§ | æ—  | å®Œæ•´æŠ¥å‘Š |

---

## âœ… æ£€æŸ¥æ¸…å•

### ä¼˜åŒ–å‰

- [ ] ç¡®è®¤å½“å‰ç‰ˆæœ¬ v1.2.0 ç¨³å®šè¿è¡Œ
- [ ] å¤‡ä»½ç°æœ‰ä»£ç 
- [ ] åˆ›å»ºä¼˜åŒ–åˆ†æ”¯ `feature/v1.3.0-optimization`

### ä¼˜åŒ–ä¸­

- [ ] æ¯ä¸ªä¼˜åŒ–é¡¹å®Œæˆåæ›´æ–°æ­¤æ–‡æ¡£
- [ ] æ¯ä¸ªä¼˜åŒ–é¡¹å®Œæˆåç¼–å†™/æ›´æ–°æµ‹è¯•
- [ ] æ¯ä¸ªä¼˜åŒ–é¡¹å®Œæˆåæ›´æ–°é…ç½®æ–‡ä»¶

### ä¼˜åŒ–å

- [ ] å®Œæ•´æµ‹è¯•å¥—ä»¶é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•è¾¾æ ‡
- [ ] æ–‡æ¡£æ›´æ–°å®Œæˆ
- [ ] CHANGELOG.md è®°å½•å˜æ›´
- [ ] å‘å¸ƒ v1.3.0

---

**å˜æ›´å†å²è®°å½•åœ¨** [æ ¹çº§ CHANGELOG.md](../../../CHANGELOG.md)
