#!/usr/bin/env python3
"""
validate_no_process_leakage.py - æ£€æµ‹ç»¼è¿°æ­£æ–‡ä¸­çš„ AI æµç¨‹æ³„éœ²

ç”¨äºŽæ£€æµ‹ç»¼è¿° LaTeX æ–‡ä»¶ä¸­æ˜¯å¦å‡ºçŽ°äº†"AIå·¥ä½œæµç¨‹"ç›¸å…³æè¿°ï¼Œ
è¿™äº›æè¿°åº”è¯¥æ”¾åœ¨ {ä¸»é¢˜}_å·¥ä½œæ¡ä»¶.md ä¸­ï¼Œè€Œéžç»¼è¿°æ­£æ–‡ã€‚

æ£€æµ‹ç›®æ ‡ï¼š
1. æ£€ç´¢æµç¨‹ç»Ÿè®¡ï¼ˆ"åŸºäºŽ X æ¡æ–‡çŒ®"ã€"åŽ»é‡åŽ Y æ¡"ç­‰ï¼‰
2. è¯„åˆ†é€‰æ–‡æµç¨‹ï¼ˆ"ç›¸å…³æ€§è¯„åˆ†ç³»ç»Ÿ"ã€"é«˜åˆ†ä¼˜å…ˆé€‰æ–‡"ç­‰ï¼‰
3. å®Œæ•´å·¥ä½œç®¡çº¿ï¼ˆ"æ£€ç´¢â†’åŽ»é‡â†’è¯„åˆ†â†’é€‰æ–‡â†’å†™ä½œ"ç­‰ï¼‰
4. å…ƒæ“ä½œæè¿°ï¼ˆ"å­—æ•°é¢„ç®—ç³»ç»Ÿ"ã€"å¤šæŸ¥è¯¢ç­–ç•¥"ç­‰ï¼‰
"""

from __future__ import annotations

import argparse
import re
import sys
from collections import defaultdict
from pathlib import Path
from typing import List, Tuple


# ============================================================================
# æ£€æµ‹æ¨¡å¼å®šä¹‰
# ============================================================================

# æ£€æµ‹æ¨¡å¼åˆ—è¡¨ï¼š(åç§°, æ­£åˆ™è¡¨è¾¾å¼, ä¸¥é‡æ€§, æè¿°)
DETECTION_PATTERNS = [
    (
        "æ–‡çŒ®ç»Ÿè®¡æ³„éœ²",
        r"æœ¬ç»¼è¿°(?:åŸºäºŽ|å…±|çº³å…¥|æ¶µç›–|åˆ†æžäº†)\s*\d+\s*æ¡?(?:åˆæ£€|å€™é€‰|æ–‡çŒ®|ç ”ç©¶|è®ºæ–‡)",
        "high",
        "æ‘˜è¦æˆ–æ­£æ–‡å‡ºçŽ°æ–‡çŒ®æ•°é‡ç»Ÿè®¡",
    ),
    (
        "åŽ»é‡ç»Ÿè®¡æ³„éœ²",
        r"åŽ»é‡(?:åŽ|æœ‰|(?:ä¿ç•™|å‰©ä½™)äº†?)\s*\d+\s*æ¡?(?:æ–‡çŒ®|ç ”ç©¶|è®ºæ–‡)",
        "high",
        "å‡ºçŽ°åŽ»é‡åŽçš„æ–‡çŒ®æ•°é‡",
    ),
    (
        "é€‰æ–‡ç»Ÿè®¡æ³„éœ²",
        r"(?:æœ€ç»ˆ|ç­›é€‰å‡º|ä¿ç•™|çº³å…¥)\s*(?:\d+\s*ç¯‡?|å‰\s*\d+\s*ç¯‡)(?:ä»£è¡¨)?(?:ç ”ç©¶|æ–‡çŒ®|è®ºæ–‡)",
        "high",
        "å‡ºçŽ°æœ€ç»ˆé€‰ä¸­çš„æ–‡çŒ®æ•°é‡",
    ),
    (
        "è¯„åˆ†æµç¨‹æ³„éœ²",
        r"(?:é‡‡ç”¨|ä½¿ç”¨|åŸºäºŽ)\s*(?:ç›¸å…³æ€§)?è¯„åˆ†\s*(?:ç³»ç»Ÿ|æœºåˆ¶|æ–¹æ³•|æ¨¡åž‹)",
        "high",
        "å‡ºçŽ°è¯„åˆ†ç³»ç»Ÿæè¿°",
    ),
    (
        "è¯„åˆ†é€‰æ–‡æ³„éœ²",
        r"é«˜åˆ†(?:ä¼˜å…ˆ)?(?:æ¯”ä¾‹)?(?:ç­›é€‰|é€‰æ–‡|é€‰æ‹©)",
        "medium",
        "å‡ºçŽ°é«˜åˆ†ä¼˜å…ˆé€‰æ–‡æè¿°",
    ),
    (
        "å·¥ä½œç®¡çº¿æ³„éœ²",
        r"(?:æ–¹æ³•å­¦(?:ä¸Š|ä¸­)?ï¼Œ?(?:æœ¬ç»¼è¿°)?)?æŒ‰ç…§[\s\S]*?(?:æ£€ç´¢|æœç´¢|æŸ¥è¯¢)[\s\S]*?(?:åŽ»é‡)[\s\S]*?(?:ç›¸å…³æ€§)?è¯„åˆ†[\s\S]*?(?:é«˜åˆ†)?(?:é€‰æ–‡|ç­›é€‰)",
        "high",
        "å‡ºçŽ°å®Œæ•´å·¥ä½œç®¡çº¿æè¿°",
    ),
    (
        "æ£€ç´¢ç­–ç•¥æ³„éœ²",
        r"(?:æ–‡çŒ®)?(?:æ£€ç´¢|æœç´¢|æŸ¥è¯¢)(?:é‡‡ç”¨|ä½¿ç”¨|åŸºäºŽ)?\s*(?:å¤šæŸ¥è¯¢|å¤šç­–ç•¥|å¹¶è¡Œ|å˜ä½“)",
        "medium",
        "å‡ºçŽ°æ£€ç´¢ç­–ç•¥æè¿°",
    ),
    (
        "æ•°æ®åº“æ³„éœ²",
        r"OpenAlex|PubMed|Web of Science|IEEE Xplore\s*(?:æ•°æ®åº“|API|æ•°æ®æº)",
        "medium",
        "å‡ºçŽ°æ•°æ®åº“åç§°ï¼ˆåº”åœ¨å·¥ä½œæ¡ä»¶ä¸­ï¼‰",
    ),
    (
        "å­—æ•°é¢„ç®—æ³„éœ²",
        r"(?:å­—æ•°|è¯æ•°)(?:é¢„ç®—|åˆ†é…|è§„åˆ’)(?:ç³»ç»Ÿ|æœºåˆ¶|æ–¹æ³•)",
        "medium",
        "å‡ºçŽ°å­—æ•°é¢„ç®—ç³»ç»Ÿæè¿°",
    ),
    (
        "æµç¨‹å…³é”®è¯",
        r"\b(?:æ£€ç´¢|åŽ»é‡|ç›¸å…³æ€§è¯„åˆ†|é€‰æ–‡|å­—æ•°é¢„ç®—)(?:æµç¨‹|ç®¡çº¿|æ­¥éª¤|é˜¶æ®µ)\b",
        "medium",
        "å‡ºçŽ°æµç¨‹å…³é”®è¯ç»„åˆ",
    ),
]


# ============================================================================
# æ ¸å¿ƒæ£€æµ‹é€»è¾‘
# ============================================================================


def load_file_content(tex_path: Path) -> str:
    """è¯»å– LaTeX æ–‡ä»¶å†…å®¹ï¼Œç§»é™¤æ³¨é‡Šå’Œçº¯ä»£ç å—"""
    if not tex_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {tex_path}")

    content = tex_path.read_text(encoding="utf-8", errors="replace")

    # ç§»é™¤ LaTeX æ³¨é‡Š
    content = re.sub(r"%.*$", "", content, flags=re.MULTILINE)

    # ç§»é™¤å¸¸è§çš„çº¯ä»£ç çŽ¯å¢ƒï¼ˆè¿™äº›ä¸åŒ…å«æ­£æ–‡å†…å®¹ï¼‰
    # ä¿ç•™ \section, \subsection, \paragraph ç­‰ç»“æž„å‘½ä»¤åŽçš„å†…å®¹
    pure_code_envs = [
        r"\\bibliography\{[^}]*\}",
        r"\\bibliographystyle\{[^}]*\}",
        r"\\begin\{thebibliography\}.*?\\end\{thebibliography\}",
        r"\\documentclass(?:\[.*?\])?\{.*?\}",
        r"\\usepackage(?:\[.*?\])?\{.*?\}",
    ]

    for env_pattern in pure_code_envs:
        content = re.sub(env_pattern, "", content, flags=re.DOTALL)

    return content


def detect_leakage(content: str, patterns: List[Tuple[str, str, str, str]]) -> defaultdict:
    """æ‰§è¡Œæ£€æµ‹å¹¶è¿”å›žç»“æžœ"""
    results = defaultdict(lambda: {"matches": [], "severity": "unknown", "description": ""})

    for name, pattern, severity, description in patterns:
        matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            # æå–åŒ¹é…è¡Œçš„ä¸Šä¸‹æ–‡
            lines_before = content[:match.start()].split("\n")
            line_num = len(lines_before)
            line_text = lines_before[-1].strip() if lines_before else ""

            results[name]["matches"].append({
                "line": line_num,
                "text": line_text[:100],  # é™åˆ¶é•¿åº¦
                "match": match.group(0),
            })
            results[name]["severity"] = severity
            results[name]["description"] = description

    return results


def format_results(results: defaultdict, total_issues: int) -> str:
    """æ ¼å¼åŒ–æ£€æµ‹ç»“æžœ"""
    if total_issues == 0:
        return "âœ… æœªæ£€æµ‹åˆ° AI æµç¨‹æ³„éœ²é—®é¢˜\n"

    output = ["\nðŸ” æ£€æµ‹åˆ° AI æµç¨‹æ³„éœ²é—®é¢˜\n", "=" * 70]

    # æŒ‰ä¸¥é‡æ€§åˆ†ç»„
    high_issues = []
    medium_issues = []

    for name, data in results.items():
        if not data["matches"]:
            continue
        issue_info = (name, data)
        if data["severity"] == "high":
            high_issues.append(issue_info)
        else:
            medium_issues.append(issue_info)

    if high_issues:
        output.append("\nðŸ”´ ä¸¥é‡é—®é¢˜ï¼ˆå¿…é¡»ä¿®å¤ï¼‰:")
        output.append("-" * 70)
        for name, data in high_issues:
            output.append(f"\nâš ï¸  {name}: {data['description']}")
            for match in data["matches"][:3]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                output.append(f"   ç¬¬ {match['line']} è¡Œ: {match['text']}")
            if len(data["matches"]) > 3:
                output.append(f"   ... è¿˜æœ‰ {len(data['matches']) - 3} å¤„åŒ¹é…")

    if medium_issues:
        output.append("\nðŸŸ¡ ä¸­ç­‰é—®é¢˜ï¼ˆå»ºè®®ä¿®å¤ï¼‰:")
        output.append("-" * 70)
        for name, data in medium_issues:
            output.append(f"\nâš ï¸  {name}: {data['description']}")
            for match in data["matches"][:3]:
                output.append(f"   ç¬¬ {match['line']} è¡Œ: {match['text']}")
            if len(data["matches"]) > 3:
                output.append(f"   ... è¿˜æœ‰ {len(data['matches']) - 3} å¤„åŒ¹é…")

    output.append("\n" + "=" * 70)
    output.append("\nðŸ’¡ ä¿®å¤å»ºè®®:")
    output.append("1. åˆ é™¤ç»¼è¿°æ­£æ–‡ä¸­çš„ä¸Šè¿°å†…å®¹")
    output.append("2. å°†æ–¹æ³•å­¦ä¿¡æ¯ç§»è‡³ {ä¸»é¢˜}_å·¥ä½œæ¡ä»¶.md çš„ç›¸åº”ç« èŠ‚")
    output.append("3. ç¡®ä¿ç»¼è¿°æ­£æ–‡å®Œå…¨èšç„¦é¢†åŸŸçŸ¥è¯†")
    output.append(f"\nè¯¦ç»†è¯´æ˜Žè§: references/expert-review-writing.md çš„'å†…å®¹åˆ†ç¦»åŽŸåˆ™'ç« èŠ‚\n")

    return "\n".join(output)


# ============================================================================
# CLI æŽ¥å£
# ============================================================================


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="æ£€æµ‹ç»¼è¿°æ­£æ–‡ä¸­çš„ AI æµç¨‹æ³„éœ²",
        epilog="ç¤ºä¾‹: python validate_no_process_leakage.py review.tex"
    )
    parser.add_argument(
        "tex_file",
        type=Path,
        help="è¦æ£€æµ‹çš„ LaTeX æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†åŒ¹é…ä¿¡æ¯",
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="ä»¥ JSON æ ¼å¼è¾“å‡ºç»“æžœ",
    )
    return parser.parse_args()


def main() -> int:
    args = parse_args()

    try:
        content = load_file_content(args.tex_file)
    except FileNotFoundError as e:
        print(f"âœ— {e}", file=sys.stderr)
        return 1
    except Exception as e:
        print(f"âœ— è¯»å–æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
        return 1

    results = detect_leakage(content, DETECTION_PATTERNS)
    total_issues = sum(len(data["matches"]) for data in results.values())

    if args.json:
        # JSON è¾“å‡ºï¼ˆç”¨äºŽè„šæœ¬é›†æˆï¼‰
        import json
        json_data = {
            "total_issues": total_issues,
            "passed": total_issues == 0,
            "issues": {
                name: {
                    "severity": data["severity"],
                    "description": data["description"],
                    "count": len(data["matches"]),
                    "matches": data["matches"] if args.verbose else [],
                }
                for name, data in results.items()
                if data["matches"]
            }
        }
        print(json.dumps(json_data, ensure_ascii=False, indent=2))
        # JSON æ¨¡å¼ä¹Ÿè¦è¿”å›žæ­£ç¡®çš„é€€å‡ºç 
        if total_issues > 0:
            return 1
        return 0
    else:
        # äººæ€§åŒ–è¾“å‡º
        print(f"\nðŸ“„ æ£€æµ‹æ–‡ä»¶: {args.tex_file.name}")
        print(format_results(results, total_issues))

        if total_issues > 0:
            print(f"æ€»è®¡: {total_issues} å¤„é—®é¢˜")
            return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
