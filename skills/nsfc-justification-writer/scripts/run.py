#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations

import argparse
import json
import logging
import sys
import traceback
import webbrowser
from pathlib import Path
from typing import Any, Dict, Optional

sys.dont_write_bytecode = True

skill_root_for_import = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(skill_root_for_import))

from core.config_loader import load_config, get_runs_dir, validate_config
from core.config_access import get_bool, get_mapping, get_seq_str, get_str
from core.bib_manager_integration import BibFixSuggestion
from core.errors import BackupNotFoundError, MissingCitationKeysError, SectionNotFoundError, SkillError
from core.html_report import render_diagnostic_html
from core.hybrid_coordinator import HybridCoordinator
from core.info_form import copy_info_form_template, interactive_collect_info_form, write_info_form_file
from core.latex_parser import parse_subsubsections
from core.logging_utils import configure_logging
from core.observability import make_run_id
from core.quality_gate import check_new_body_quality
from core.versioning import find_backup_for_run, list_runs, rollback_from_backup, unified_diff

logger = logging.getLogger(__name__)


def _write_json(path: Path, data: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")


def _read_body_file(body_file: Optional[str]) -> str:
    if body_file is None or body_file == "-":
        return sys.stdin.read()
    return Path(body_file).read_text(encoding="utf-8", errors="ignore")


def _load_config_for_args(skill_root: Path, args: argparse.Namespace) -> Dict[str, Any]:
    preset = getattr(args, "preset", None)
    override = getattr(args, "override", None)
    no_user_override = bool(getattr(args, "no_user_override", False))
    cfg = load_config(
        skill_root,
        preset=str(preset) if preset else None,
        override_path=str(override) if override else None,
        load_user_override=(not no_user_override),
    )
    meta = get_mapping(cfg, "_config_loader")
    warnings = list(meta.get("warnings", []) or [])
    for w in warnings[:10]:
        logger.warning("âš ï¸ é…ç½®åŠ è½½è­¦å‘Šï¼š%s", w)
    if len(warnings) > 10:
        logger.warning("âš ï¸ é…ç½®åŠ è½½è­¦å‘Šï¼šæ›´å¤š %s æ¡å·²çœç•¥", str(len(warnings) - 10))
    return cfg


def cmd_diagnose(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)

    if args.tier2 and getattr(args, "verbose", False):
        logger.info("â³ æ­£åœ¨è¿è¡Œè¯Šæ–­ï¼ˆå« Tier2ï¼‰...")
    report = coord.diagnose(
        project_root=Path(args.project_root),
        include_tier2=bool(args.tier2),
        tier2_chunk_size=int(args.chunk_size) if args.chunk_size is not None else None,
        tier2_max_chunks=int(args.max_chunks) if args.max_chunks is not None else None,
        tier2_fresh=bool(getattr(args, "fresh", False)),
    )
    text = coord.format_diagnose(report)
    print(text, end="")

    if args.json_out:
        _write_json(Path(args.json_out), report.to_dict())

    if args.html_report:
        if getattr(args, "verbose", False):
            logger.info("â³ æ­£åœ¨ç”Ÿæˆ HTML æŠ¥å‘Š...")
        run_id = args.run_id or make_run_id("diagnose")
        runs_root = get_runs_dir(skill_root, config)
        out_path = Path(args.html_report)
        if str(args.html_report).strip().lower() == "auto":
            out_path = (runs_root / run_id / "reports" / "diagnose.html").resolve()

        targets = get_mapping(config, "targets")
        target_relpath = get_str(targets, "justification_tex", "extraTex/1.1.ç«‹é¡¹ä¾æ®.tex")
        target = coord.target_path(project_root=Path(args.project_root))
        tex = target.read_text(encoding="utf-8", errors="ignore") if target.exists() else ""
        include_terms = not bool(getattr(args, "no_terms", False))
        term_md = coord.term_consistency_report(project_root=Path(args.project_root)) if include_terms else ""
        html_text = render_diagnostic_html(
            skill_root=skill_root,
            project_root=Path(args.project_root),
            target_relpath=target_relpath,
            tex_text=tex,
            report=report,
            term_matrix_md=term_md,
        )
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(html_text, encoding="utf-8")
        print(f"ğŸ§© HTML æŠ¥å‘Šï¼š{out_path}")
        if bool(getattr(args, "open", False)):
            try:
                webbrowser.open(out_path.resolve().as_uri())
            except (OSError, ValueError, webbrowser.Error) as e:
                if bool(getattr(args, "verbose", False)):
                    logger.warning("âš ï¸ æ‰“å¼€æµè§ˆå™¨å¤±è´¥ï¼š%s: %s", type(e).__name__, str(e))
    return 0


def cmd_wordcount(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)
    status = coord.word_count_status(project_root=Path(args.project_root), mode=getattr(args, "mode", None))
    print(json.dumps(status, ensure_ascii=False, indent=2))
    return 0


def cmd_refs(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)

    report = coord.diagnose(project_root=Path(args.project_root), include_tier2=False)
    sug = BibFixSuggestion(
        missing_bibkeys=list(report.tier1.missing_citation_keys or []),
        missing_doi_keys=list(getattr(report.tier1, "missing_doi_keys", []) or []),
        invalid_doi_keys=list(getattr(report.tier1, "invalid_doi_keys", []) or []),
    )
    md = sug.to_markdown(project_root=str(Path(args.project_root)))
    if str(getattr(args, "verify_doi", "none")).strip().lower() == "crossref":
        from core.reference_validator import load_project_bib_doi_map, parse_cite_keys, verify_doi_via_crossref

        target = coord.target_path(project_root=Path(args.project_root))
        tex = target.read_text(encoding="utf-8", errors="ignore") if target.exists() else ""
        cite_keys = parse_cite_keys(tex)
        targets = get_mapping(config, "targets")
        bib_globs = list(get_seq_str(targets, "bib_globs")) or ["references/*.bib"]
        doi_map = load_project_bib_doi_map(Path(args.project_root), bib_globs)
        pairs = [(k, doi_map.get(k, "")) for k in cite_keys if doi_map.get(k)]

        failed = []
        timeout_s = float(getattr(args, "doi_timeout", 5.0))
        for k, doi in pairs[:200]:
            ok = verify_doi_via_crossref(doi=doi, timeout_s=timeout_s)
            if not ok:
                failed.append(f"- {k}: {doi}")
        if failed:
            md = md.rstrip() + "\n\n## Crossrefï¼ˆå¯é€‰è”ç½‘ï¼‰æ ¡éªŒå¤±è´¥/è¶…æ—¶çš„ DOIï¼ˆéœ€äººå·¥æ ¸éªŒï¼‰\n\n" + "\n".join(failed) + "\n"
        else:
            md = md.rstrip() + "\n\n## Crossrefï¼ˆå¯é€‰è”ç½‘ï¼‰æ ¡éªŒ\n\n- âœ… æœªå‘ç°æ˜æ˜¾å¤±è´¥ï¼ˆä»å»ºè®®æŠ½æŸ¥å…³é”®å¼•ç”¨ï¼‰\n"
    if args.out:
        Path(args.out).write_text(md, encoding="utf-8")
        print(f"å·²è¾“å‡ºï¼š{args.out}")
        return 0
    print(md, end="")
    return 0


def cmd_terms(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)
    md = coord.term_consistency_report(project_root=Path(args.project_root))
    targets = get_mapping(config, "targets")
    related = get_mapping(targets, "related_tex")
    if related:
        md = (
            md.rstrip()
            + "\n\n## å»ºè®®åŒæ­¥åˆ°ä»¥ä¸‹ç« èŠ‚\n\n"
            + "\n".join([f"- {k}: `{v}`" for k, v in related.items()])
            + "\n"
        )
    if args.out:
        Path(args.out).write_text(md, encoding="utf-8")
        print(f"å·²è¾“å‡ºï¼š{args.out}")
        return 0
    print(md, end="")
    return 0


def cmd_validate_config(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    errs = validate_config(skill_root=skill_root, config=config)
    if errs:
        logger.error("âŒ é…ç½®æ ¡éªŒå¤±è´¥ï¼š")
        for e in errs:
            logger.error("- %s", e)
        return 2
    print("âœ… é…ç½®æœ‰æ•ˆ")
    return 0


def cmd_apply_section(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)

    body = _read_body_file(args.body_file).strip()
    if not body:
        logger.error("âŒ body ä¸ºç©ºï¼šè¯·é€šè¿‡ --body-file æˆ– stdin æä¾›æ–°æ­£æ–‡")
        return 2

    run_id = args.run_id or make_run_id("apply")
    # è‹¥ç”¨æˆ·é€‰æ‹©æ”¾å®½å¼•ç”¨çº¦æŸï¼Œå»ºè®®è‡³å°‘å¯ç”¨â€œæ–°æ­£æ–‡è´¨é‡é—¸é—¨â€ï¼ˆå¯é€‰é˜»æ–­ï¼‰ã€‚
    if bool(getattr(args, "allow_missing_citations", False)) and (not bool(getattr(args, "strict_quality", False))):
        strict_cfg = get_bool(get_mapping(config, "quality"), "strict_on_apply", False)
        if not strict_cfg:
            qr = check_new_body_quality(new_body=body, config=config)
            if not qr.ok:
                if qr.forbidden_phrases_hits:
                    logger.warning(
                        "âš ï¸ æ–°æ­£æ–‡åŒ…å«ä¸å¯æ ¸éªŒè¡¨è¿°ï¼ˆå»ºè®®æ”¹å†™æˆ–å¯ç”¨ --strict-quality é˜»æ–­å†™å…¥ï¼‰ï¼š%s",
                        "ã€".join(qr.forbidden_phrases_hits[:10]),
                    )
                if qr.avoid_commands_hits:
                    logger.warning(
                        "âš ï¸ æ–°æ­£æ–‡åŒ…å«å¯èƒ½ç ´åæ¨¡æ¿çš„å‘½ä»¤ï¼ˆå»ºè®®ç§»é™¤æˆ–å¯ç”¨ --strict-quality é˜»æ–­å†™å…¥ï¼‰ï¼š%s",
                        "ã€".join(qr.avoid_commands_hits[:10]),
                    )
    try:
        result = coord.apply_section_body(
            project_root=Path(args.project_root),
            title=args.title,
            new_body=body,
            backup=not bool(args.no_backup),
            run_id=run_id,
            allow_missing_citations=bool(args.allow_missing_citations),
            strict_quality=bool(getattr(args, "strict_quality", False)),
        )
    except MissingCitationKeysError as e:
        logger.error("âŒ %s", str(e))
        if e.missing_keys:
            logger.error("\nç¼ºå¤±çš„ bibkeyï¼š")
            for k in e.missing_keys[:50]:
                logger.error("- %s", k)
        if getattr(e, "fix_suggestion", ""):
            logger.error("\nğŸ’¡ ä¿®å¤å»ºè®®ï¼š")
            logger.error("%s", getattr(e, "fix_suggestion", ""))
        return 2
    except SectionNotFoundError as e:
        logger.error("âŒ %s", str(e))
        if getattr(e, "fix_suggestion", ""):
            logger.error("\nğŸ’¡ ä¿®å¤å»ºè®®ï¼š")
            logger.error("%s", getattr(e, "fix_suggestion", ""))
        if bool(getattr(args, "suggest_alias", False)):
            target = coord.target_path(project_root=Path(args.project_root))
            if target.exists():
                tex = target.read_text(encoding="utf-8", errors="ignore")
                titles = [s.title for s in parse_subsubsections(tex)]
                if titles:
                    logger.error("\nå¯ç”¨çš„å°æ ‡é¢˜ï¼ˆå…¨éƒ¨ï¼‰ï¼š")
                    for t in titles[:80]:
                        logger.error("- %s", t)
        return 2

    print(f"âœ… å·²å†™å…¥ï¼š{result.target_path}")
    if result.backup_path:
        print(f"ğŸ“¦ å¤‡ä»½ï¼š{result.backup_path}")

    if args.log_json:
        runs_root = get_runs_dir(skill_root, config)
        log_path = (runs_root / run_id / "logs" / "apply_result.json").resolve()
        _write_json(
            log_path,
            {
                "run_id": run_id,
                "target": str(result.target_path),
                "backup": str(result.backup_path) if result.backup_path else None,
            },
        )
        print(f"ğŸ§¾ è®°å½•ï¼š{log_path}")
    return 0


def cmd_init(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    version = get_str(get_mapping(config, "skill_info"), "version", "")
    runs_root = get_runs_dir(skill_root, config)
    run_id = args.run_id or make_run_id("init")

    out_path = Path(args.out) if args.out else (runs_root / run_id / "inputs" / "info_form.md")
    out_path = out_path.resolve()

    template_path = (skill_root / "references" / "info_form.md").resolve()
    if not args.interactive:
        ok = copy_info_form_template(template_path=template_path, out_path=out_path)
        if not ok:
            logger.error("âŒ æœªæ‰¾åˆ° info_form æ¨¡æ¿ã€‚")
            return 2
        print(f"âœ… å·²ç”Ÿæˆä¿¡æ¯è¡¨æ¨¡æ¿ï¼š{out_path}")
        return 0

    print("è¿›å…¥äº¤äº’å¼ä¿¡æ¯è¡¨æ”¶é›†ï¼ˆä»…æœ¬åœ°ç”Ÿæˆï¼Œä¸ä¼šä¿®æ”¹æ ‡ä¹¦é¡¹ç›®ç›®å½•ï¼‰ã€‚")
    answers = interactive_collect_info_form()
    write_info_form_file(out_path=out_path, answers=answers, version=version or "v0.0.0")
    print(f"âœ… å·²ç”Ÿæˆä¿¡æ¯è¡¨ï¼š{out_path}")
    return 0


def cmd_review(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)
    md = coord.reviewer_advice(
        project_root=Path(args.project_root),
        include_tier2=bool(args.tier2),
        tier2_chunk_size=int(args.chunk_size) if args.chunk_size is not None else None,
        tier2_max_chunks=int(args.max_chunks) if args.max_chunks is not None else None,
        tier2_fresh=bool(getattr(args, "fresh", False)),
    )
    if args.out:
        Path(args.out).write_text(md, encoding="utf-8")
        print(f"å·²è¾“å‡ºï¼š{args.out}")
        return 0
    print(md, end="")
    return 0


def cmd_coach(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)
    info_form_text = ""
    if args.info_form:
        info_form_text = Path(args.info_form).read_text(encoding="utf-8", errors="ignore")
    md = coord.coach(project_root=Path(args.project_root), stage=str(args.stage), info_form_text=info_form_text)
    if args.topic:
        md = coord.recommend_examples(query=str(args.topic), top_k=int(args.top_k)) + "\n" + md
    if args.out:
        Path(args.out).write_text(md, encoding="utf-8")
        print(f"å·²è¾“å‡ºï¼š{args.out}")
        return 0
    print(md, end="")
    return 0


def cmd_examples(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)
    print(coord.recommend_examples(query=str(args.query), top_k=int(args.top_k)), end="")
    return 0


def cmd_list_runs(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    runs_root = get_runs_dir(skill_root, config)
    runs = list_runs(runs_root=runs_root)
    if not runs:
        print("ï¼ˆæš‚æ—  runs è®°å½•ï¼‰")
        return 0
    for r in runs[: int(args.limit)]:
        print(r.run_id)
    return 0


def cmd_diff(args: argparse.Namespace) -> int:
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)
    runs_root = get_runs_dir(skill_root, config)
    target = coord.target_path(project_root=Path(args.project_root))
    try:
        backup = find_backup_for_run(runs_root=runs_root, run_id=str(args.run_id), filename=target.name)
    except BackupNotFoundError:
        logger.error("âŒ æœªæ‰¾åˆ° run_id=%s çš„å¤‡ä»½æ–‡ä»¶ã€‚", str(args.run_id))
        return 2
    old = backup.read_text(encoding="utf-8", errors="ignore")
    new = target.read_text(encoding="utf-8", errors="ignore") if target.exists() else ""
    diff_text = unified_diff(
        old_text=old,
        new_text=new,
        fromfile=str(backup),
        tofile=str(target),
        context_lines=int(args.context),
    )
    print(diff_text, end="")
    return 0


def cmd_rollback(args: argparse.Namespace) -> int:
    if not args.yes:
        logger.error("âŒ å›æ»šéœ€è¦æ˜¾å¼ç¡®è®¤ï¼šè¯·åŠ  --yes")
        return 2
    skill_root = Path(__file__).resolve().parent.parent
    config = _load_config_for_args(skill_root, args)
    coord = HybridCoordinator(skill_root=skill_root, config=config)
    runs_root = get_runs_dir(skill_root, config)
    target = coord.target_path(project_root=Path(args.project_root))
    try:
        used = rollback_from_backup(
            runs_root=runs_root,
            run_id=str(args.run_id),
            target_path=target,
            backup_current=not bool(args.no_backup),
            rollback_run_id=args.new_run_id,
        )
    except BackupNotFoundError:
        logger.error("âŒ æœªæ‰¾åˆ° run_id=%s çš„å¤‡ä»½æ–‡ä»¶ã€‚", str(args.run_id))
        return 2
    print(f"âœ… å·²å›æ»šï¼š{target}")
    print(f"ğŸ“¦ ä½¿ç”¨å¤‡ä»½ï¼š{used}")
    return 0


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="nsfc-justification-writer", add_help=True)
    p.add_argument("--verbose", action="store_true", help="è¾“å‡ºæ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ˆåŒ…å«å †æ ˆï¼‰")
    p.add_argument("--preset", help="åŠ è½½ config/presets/<name>.yamlï¼ˆå¯é€‰ï¼‰")
    p.add_argument("--override", help="é¢å¤–é…ç½®è¦†ç›–æ–‡ä»¶ï¼ˆyamlï¼Œå¯é€‰ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰")
    p.add_argument("--no-user-override", action="store_true", help="ä¸åŠ è½½ ~/.config/nsfc-justification-writer/override.yaml")
    sub = p.add_subparsers(dest="cmd", required=True)

    p_diag = sub.add_parser("diagnose", help="Tier1/Tier2 è¯Šæ–­ï¼ˆç»“æ„/å¼•ç”¨/å­—æ•°/è¡¨è¿°ï¼‰")
    p_diag.add_argument("--project-root", required=True)
    p_diag.add_argument("--tier2", action="store_true", help="å¯ç”¨ AI Tier2ï¼ˆéœ€è¦ responder ç¯å¢ƒï¼‰")
    p_diag.add_argument("--chunk-size", type=int, default=12000, help="Tier2 åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œç”¨äºå¤§æ–‡ä»¶ï¼›<=0 è¡¨ç¤ºä¸åˆ†å—")
    p_diag.add_argument("--max-chunks", type=int, default=20, help="Tier2 æœ€å¤šå¤„ç†çš„åˆ†å—æ•°ï¼ˆé˜²æ­¢è¶…é•¿æ–‡ä»¶è¿‡æ…¢ï¼‰")
    p_diag.add_argument("--fresh", action="store_true", help="å¿½ç•¥ AI ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°è®¡ç®— Tier2")
    p_diag.add_argument("--json-out", help="å¯é€‰ï¼šè¾“å‡º JSON æŠ¥å‘Šåˆ°æ–‡ä»¶")
    p_diag.add_argument("--html-report", help="å¯é€‰ï¼šè¾“å‡º HTML æŠ¥å‘Šåˆ°æ–‡ä»¶ï¼›ç”¨ auto è¾“å‡ºåˆ° runs/...")
    p_diag.add_argument("--open", action="store_true", help="è‹¥ç”Ÿæˆ HTML æŠ¥å‘Šåˆ™å°è¯•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨")
    p_diag.add_argument("--no-terms", action="store_true", help="HTML æŠ¥å‘Šä¸é™„å¸¦æœ¯è¯­ä¸€è‡´æ€§çŸ©é˜µ")
    p_diag.add_argument("--run-id", help="å¯é€‰ï¼šdiagnose çš„ run_idï¼ˆç”¨äº html-report=autoï¼‰")
    p_diag.set_defaults(func=cmd_diagnose)

    p_wc = sub.add_parser("wordcount", help="ç»Ÿè®¡ 1.1 ç«‹é¡¹ä¾æ®å­—æ•°å¹¶ç»™å‡ºåå·®")
    p_wc.add_argument("--project-root", required=True)
    p_wc.add_argument(
        "--mode",
        default=None,
        choices=["cjk_only", "cjk_strip_commands"],
        help="ç»Ÿè®¡å£å¾„ï¼šcjk_onlyï¼ˆé»˜è®¤ï¼‰æˆ– cjk_strip_commandsï¼ˆæ›´æ¥è¿‘æ­£æ–‡ä¼°è®¡ï¼‰",
    )
    p_wc.set_defaults(func=cmd_wordcount)

    p_refs = sub.add_parser("refs", help="å¼•ç”¨æ ¸éªŒæ‘˜è¦ + ç”Ÿæˆ nsfc-bib-manager å¯å¤åˆ¶æç¤ºè¯")
    p_refs.add_argument("--project-root", required=True)
    p_refs.add_argument("--verify-doi", default="none", choices=["none", "crossref"], help="å¯é€‰ï¼šè”ç½‘ç”¨ Crossref æ ¡éªŒ DOI")
    p_refs.add_argument("--doi-timeout", default=5.0, type=float, help="Crossref æ ¡éªŒè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    p_refs.add_argument("--out", help="å¯é€‰ï¼šè¾“å‡ºåˆ°æ–‡ä»¶ï¼ˆmarkdownï¼‰")
    p_refs.set_defaults(func=cmd_refs)

    p_terms = sub.add_parser("terms", help="æœ¯è¯­ä¸€è‡´æ€§ï¼ˆç¡¬ç¼–ç  alias_groupsï¼‰")
    p_terms.add_argument("--project-root", required=True)
    p_terms.add_argument("--out", help="å¯é€‰ï¼šè¾“å‡ºåˆ°æ–‡ä»¶ï¼ˆmarkdownï¼‰")
    p_terms.set_defaults(func=cmd_terms)

    p_init = sub.add_parser("init", help="ç”Ÿæˆï¼ˆæˆ–äº¤äº’å¼å¡«å†™ï¼‰ä¿¡æ¯è¡¨ info_form.md")
    p_init.add_argument("--interactive", action="store_true", help="é—®ç­”å¼æ”¶é›†å¹¶ç”Ÿæˆå·²å¡«å†™çš„ä¿¡æ¯è¡¨")
    p_init.add_argument("--out", help="è¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤å†™åˆ° runs/<run_id>/inputs/info_form.mdï¼‰")
    p_init.add_argument("--run-id", help="å¯é€‰ï¼šæŒ‡å®š run_idï¼ˆé»˜è®¤æŒ‰æ—¶é—´ç”Ÿæˆï¼‰")
    p_init.set_defaults(func=cmd_init)

    p_review = sub.add_parser("review", help="è¯„å®¡äººè§†è§’è´¨ç–‘ä¸å»ºè®®ï¼ˆå¯é€‰ Tier2ï¼‰")
    p_review.add_argument("--project-root", required=True)
    p_review.add_argument("--tier2", action="store_true", help="å¯ç”¨ AI Tier2ï¼ˆéœ€è¦ responder ç¯å¢ƒï¼‰")
    p_review.add_argument("--chunk-size", type=int, default=12000, help="Tier2 åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œç”¨äºå¤§æ–‡ä»¶ï¼›<=0 è¡¨ç¤ºä¸åˆ†å—")
    p_review.add_argument("--max-chunks", type=int, default=20, help="Tier2 æœ€å¤šå¤„ç†çš„åˆ†å—æ•°ï¼ˆé˜²æ­¢è¶…é•¿æ–‡ä»¶è¿‡æ…¢ï¼‰")
    p_review.add_argument("--fresh", action="store_true", help="å¿½ç•¥ AI ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°è®¡ç®— Tier2")
    p_review.add_argument("--out", help="å¯é€‰ï¼šè¾“å‡ºåˆ°æ–‡ä»¶ï¼ˆmarkdownï¼‰")
    p_review.set_defaults(func=cmd_review)

    p_coach = sub.add_parser("coach", help="æ¸è¿›å¼å†™ä½œå¼•å¯¼ï¼ˆéª¨æ¶â†’æ®µè½â†’ä¿®è®¢â†’æ¶¦è‰²â†’éªŒæ”¶ï¼‰")
    p_coach.add_argument("--project-root", required=True)
    p_coach.add_argument("--stage", default="auto", choices=["auto", "skeleton", "draft", "revise", "polish", "final"])
    p_coach.add_argument("--info-form", help="å¯é€‰ï¼šå·²å¡«å†™çš„ä¿¡æ¯è¡¨æ–‡ä»¶ï¼ˆmarkdownï¼‰")
    p_coach.add_argument("--topic", help="å¯é€‰ï¼šä¸€å¥è¯ä¸»é¢˜ï¼Œç”¨äºæ¨è examples/ ç¤ºä¾‹")
    p_coach.add_argument("--top-k", default=3, type=int)
    p_coach.add_argument("--out", help="å¯é€‰ï¼šè¾“å‡ºåˆ°æ–‡ä»¶ï¼ˆmarkdownï¼‰")
    p_coach.set_defaults(func=cmd_coach)

    p_ex = sub.add_parser("examples", help="æ ¹æ®ä¸»é¢˜æ¨è examples/ ä¸­çš„å‚è€ƒéª¨æ¶")
    p_ex.add_argument("--query", required=True, help="ä¸»é¢˜/æ–¹å‘/å…³é”®è¯")
    p_ex.add_argument("--top-k", default=3, type=int)
    p_ex.set_defaults(func=cmd_examples)

    p_runs = sub.add_parser("list-runs", help="åˆ—å‡º runs/ ä¸‹çš„ run_idï¼ˆç”¨äº diff/rollbackï¼‰")
    p_runs.add_argument("--limit", default=20, type=int)
    p_runs.set_defaults(func=cmd_list_runs)

    p_diff = sub.add_parser("diff", help="æŸ¥çœ‹æŸæ¬¡ run çš„å¤‡ä»½ä¸å½“å‰æ–‡ä»¶çš„ diff")
    p_diff.add_argument("--project-root", required=True)
    p_diff.add_argument("--run-id", required=True)
    p_diff.add_argument("--context", default=3, type=int)
    p_diff.set_defaults(func=cmd_diff)

    p_rb = sub.add_parser("rollback", help="ä»æŸæ¬¡ run çš„å¤‡ä»½å›æ»šå½“å‰æ–‡ä»¶ï¼ˆé»˜è®¤ä¼šå¤‡ä»½å½“å‰ç‰ˆæœ¬ï¼‰")
    p_rb.add_argument("--project-root", required=True)
    p_rb.add_argument("--run-id", required=True)
    p_rb.add_argument("--yes", action="store_true", help="ç¡®è®¤å›æ»šï¼ˆå¿…é¡»æ˜¾å¼æŒ‡å®šï¼‰")
    p_rb.add_argument("--no-backup", action="store_true", help="ä¸å¤‡ä»½å½“å‰ç‰ˆæœ¬ï¼ˆé»˜è®¤å¤‡ä»½åˆ°æ–°çš„ runs/ï¼‰")
    p_rb.add_argument("--new-run-id", help="å¯é€‰ï¼šå›æ»šå¤‡ä»½çš„ run_idï¼ˆé»˜è®¤æŒ‰æ—¶é—´ç”Ÿæˆï¼‰")
    p_rb.set_defaults(func=cmd_rollback)

    p_apply = sub.add_parser("apply-section", help="æ›¿æ¢æŒ‡å®š \\subsubsection çš„æ­£æ–‡ï¼ˆå®‰å…¨å†™å…¥+å¤‡ä»½ï¼‰")
    p_apply.add_argument("--project-root", required=True)
    p_apply.add_argument("--title", required=True, help="ç²¾ç¡®åŒ¹é… \\subsubsection{title}")
    p_apply.add_argument("--body-file", help="æ–°æ­£æ–‡æ¥æºæ–‡ä»¶ï¼›ç”¨ - è¡¨ç¤ºä» stdin è¯»")
    p_apply.add_argument("--no-backup", action="store_true", help="ä¸åšå¤‡ä»½ï¼ˆé»˜è®¤å¤‡ä»½ï¼‰")
    p_apply.add_argument("--run-id", help="å¯é€‰ï¼šæŒ‡å®š run_idï¼ˆé»˜è®¤æŒ‰æ—¶é—´ç”Ÿæˆï¼‰")
    p_apply.add_argument("--log-json", action="store_true", help="å†™å…¥ runs/.../logs/apply_result.json")
    p_apply.add_argument("--allow-missing-citations", action="store_true", help="å…è®¸å­˜åœ¨ç¼ºå¤± bibkey çš„ \\cite{...}ï¼ˆä¸æ¨èï¼‰")
    p_apply.add_argument("--strict-quality", action="store_true", help="å¯ç”¨â€œæ–°æ­£æ–‡è´¨é‡é—¸é—¨â€ï¼šå‘½ä¸­ç»å¯¹åŒ–è¡¨è¿°/å±é™©å‘½ä»¤åˆ™æ‹’ç»å†™å…¥")
    p_apply.add_argument("--suggest-alias", action="store_true", help="å½“æ ‡é¢˜æœªå‘½ä¸­æ—¶ï¼Œè¾“å‡ºå¯ç”¨æ ‡é¢˜å€™é€‰ï¼ˆä¾¿äºæ”¹ titleï¼‰")
    p_apply.set_defaults(func=cmd_apply_section)

    p_cfg = sub.add_parser("validate-config", help="æ ¡éªŒå½“å‰é…ç½®ï¼ˆé»˜è®¤é…ç½® + preset + overrideï¼‰")
    p_cfg.set_defaults(func=cmd_validate_config)

    return p


def main(argv: Optional[list[str]] = None) -> int:
    args = build_parser().parse_args(argv)
    configure_logging(verbose=bool(getattr(args, "verbose", False)))
    try:
        return int(args.func(args))
    except SystemExit:
        raise
    except SkillError as e:
        logger.error("âŒ %s", str(e))
        if getattr(e, "fix_suggestion", ""):
            logger.error("\nğŸ’¡ ä¿®å¤å»ºè®®ï¼š")
            logger.error("%s", getattr(e, "fix_suggestion", ""))
        return 2
    except Exception as e:
        if bool(getattr(args, "verbose", False)):
            traceback.print_exc()
            raise
        logger.error("âŒ %s: %s", type(e).__name__, str(e))
        logger.error("å»ºè®®ï¼šåŠ  --verbose æŸ¥çœ‹è¯¦ç»†å †æ ˆï¼›æˆ–å…ˆè¿è¡Œ validate-config æ£€æŸ¥é…ç½®ã€‚")
        return 2


if __name__ == "__main__":
    raise SystemExit(main())
